{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ana-AlonsoCanizares/AA_GRUPO3/blob/main/PFG_RNN_Predictiva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CouywWAlovYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "017747c3-38ba-428a-be86-6fa7befcfde6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf__oEyxOkZI",
        "outputId": "39ba6441-b16a-4ed5-c6aa-818c598e4508"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "import matplotlib.pyplot as plt\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "0InrTb-CGMCI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "folder_path = '/content/drive/My Drive/PFG_FASPAS/RNN'\n",
        "docs_xlsx = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
        "print(docs_xlsx)"
      ],
      "metadata": {
        "id": "zj2viAzoGOoP",
        "outputId": "2775e845-f676-45e5-cc81-27ada4971897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RN_LOG.xlsx', 'RN_ACT.xlsx', 'RN_EM.xlsx', 'RN_SO.xlsx', 'RN_AT.xlsx', 'RN_SAAF.xlsx', 'df_8000_rows.xlsx']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_dataframes = {}\n",
        "\n",
        "for doc in docs_xlsx:\n",
        "    entire_path = os.path.join(folder_path, doc)\n",
        "    df = pd.read_excel(entire_path)\n",
        "    # Uso el nombre del archivo como clave\n",
        "    dic_dataframes[doc] = df\n",
        "\n",
        "# Imprimir todas las claves\n",
        "for clave in dic_dataframes.keys():\n",
        "    print(clave)"
      ],
      "metadata": {
        "id": "C7_AhL9OGamp",
        "outputId": "04829501-dbac-4fd8-cfaa-ad27e6dffdc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RN_LOG.xlsx\n",
            "RN_ACT.xlsx\n",
            "RN_EM.xlsx\n",
            "RN_SO.xlsx\n",
            "RN_AT.xlsx\n",
            "RN_SAAF.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RN_SAAF = dic_dataframes.get(\"RN_SAAF.xlsx\")\n",
        "RN_AT = dic_dataframes.get(\"RN_AT.xlsx\")\n",
        "# RN_ACT = dic_dataframes.get(\"RN_ACT.xlsx\")\n",
        "RN_EM = dic_dataframes.get(\"RN_EM.xlsx\")\n",
        "RN_SO = dic_dataframes.get(\"RN_SO.xlsx\")\n",
        "RN_LOG = dic_dataframes.get(\"RN_LOG.xlsx\")\n",
        "df_8000rows = dic_dataframes.get(\"df_8000_rows.xlsx\")"
      ],
      "metadata": {
        "id": "jI7CuaujGm5D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÓDIGO SOLO PARA EL CURRO (PQ NO PUEDO ACCEDER AL DRIVE)\n",
        "RN_SAAF = pd.read_excel('/content/RN_SAAF.xlsx')\n",
        "RN_AT = pd.read_excel('/content/RN_AT.xlsx')\n",
        "# RN_ACT = pd.read_excel('/content/RN_ACT.xlsx')\n",
        "RN_EM = pd.read_excel('/content/RN_EM.xlsx')\n",
        "RN_LOG = pd.read_excel('/content/RN_LOG.xlsx')\n",
        "RN_SO = pd.read_excel('/content/RN_SO.xlsx')\n",
        "df_8000rows = pd.read_excel('/content/df_8000_rows.xlsx')\n",
        "provincia_municipio = pd.read_excel('/content/PROVINCIA_MUNICIPIO.xlsx')"
      ],
      "metadata": {
        "id": "MYSRNK9UHEcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all string columns to uppercase and remove accents/diacritics in combined_df\n",
        "df_8000rows = df_8000rows.applymap(lambda x: unidecode(x.upper()) if isinstance(x, str) else x)\n",
        "provincia_municipio = provincia_municipio.applymap(lambda x: unidecode(x.upper()) if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "5vJYmkkQMvzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add PROVINCIA column based on LOCALIDAD\n",
        "def add_provincia_column(df, provincia_mapping):\n",
        "    df['PROVINCIA'] = df['LOCALIDAD'].map(provincia_mapping)\n",
        "    return df\n",
        "\n",
        "# Create a dictionary mapping from LOCALIDAD to PROVINCIA\n",
        "localidad_to_provincia = provincia_municipio.set_index('MUNICIPIO')['PROVINCIA'].to_dict()\n",
        "\n",
        "# List of RN dataframes to process\n",
        "rn_dataframes = {'AT': RN_AT, 'EM': RN_EM, 'LOG': RN_LOG, 'SAAF': RN_SAAF, 'SO': RN_SO}\n",
        "\n",
        "# Add the PROVINCIA column to each RN dataframe\n",
        "rn_dfs_with_provincia = {name: add_provincia_column(df, localidad_to_provincia) for name, df in rn_dataframes.items()}\n",
        "\n",
        "# Drop rows with null values in the 'PROVINCIA' column in each RN dataframe\n",
        "rn_dfs_cleaned = {name: df.dropna(subset=['PROVINCIA']) for name, df in rn_dfs_with_provincia.items()}"
      ],
      "metadata": {
        "id": "njHewFYSS2NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns to keep\n",
        "required_columns = ['LOCALIDAD', 'GENERO', 'YEAR NACIMIENTO', 'PROVINCIA']\n",
        "\n",
        "# Function to ensure the dataframe has all required columns\n",
        "def ensure_columns(df, columns):\n",
        "    for column in columns:\n",
        "        if column not in df.columns:\n",
        "            df[column] = np.nan  # Create column with NaN values if it does not exist\n",
        "    return df[columns]  # Return dataframe with only the required columns\n",
        "\n",
        "# Ensure all dataframes have the required columns\n",
        "rn_dfs_final = {name: ensure_columns(df, required_columns) for name, df in rn_dfs_cleaned.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-mAp8jrXUk6",
        "outputId": "23640bbb-3821-4f2e-e2a5-152ab5992294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-109-605fc2068a84>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[column] = np.nan  # Create column with NaN values if it does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping dictionary for normalizing gender values\n",
        "gender_mapping = {\n",
        "    'FEMENINO': 'MUJER',\n",
        "    'M': 'MUJER',\n",
        "    'MASCULINO': 'HOMBRE',\n",
        "    'H': 'HOMBRE'\n",
        "}\n",
        "\n",
        "# Function to normalize the 'GENERO' column in a dataframe\n",
        "def normalize_gender_column(df):\n",
        "    df['GENERO'] = df['GENERO'].map(lambda x: gender_mapping.get(x, x))\n",
        "    return df\n",
        "\n",
        "# Apply the normalization to the 'SO' dataframe\n",
        "rn_dfs_final['SO'] = normalize_gender_column(rn_dfs_final['SO'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-1iPbJ5ZTPg",
        "outputId": "0abab4f8-e00a-4aa0-8600-d3a3016f946b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-110-333b05a14062>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['GENERO'] = df['GENERO'].map(lambda x: gender_mapping.get(x, x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to insert random registers into a dataframe where the 'GENERO' column is empty\n",
        "def insert_random_registers(df, gender_distribution):\n",
        "    # Find the indices where 'GENERO' is empty\n",
        "    empty_indices = df[df['GENERO'].isna()].index\n",
        "\n",
        "    # Generate random genders based on the given distribution for the empty indices\n",
        "    genders = np.random.choice(\n",
        "        ['MUJER', 'HOMBRE'],\n",
        "        size=len(empty_indices),\n",
        "        p=[gender_distribution['MUJER'], gender_distribution['HOMBRE']]\n",
        "    )\n",
        "\n",
        "    # Assign these random genders to the empty 'GENERO' rows\n",
        "    df.loc[empty_indices, 'GENERO'] = genders\n",
        "    return df\n",
        "\n",
        "# Example of usage\n",
        "gender_distribution = {'MUJER': 0.44, 'HOMBRE': 0.56}\n",
        "\n",
        "# Insert random registers into a specific dataframe, e.g., 'ACT', where 'GENERO' is empty\n",
        "rn_dfs_final['SAAF'] = insert_random_registers(rn_dfs_final['SAAF'], gender_distribution)\n",
        "rn_dfs_final['SO'] = insert_random_registers(rn_dfs_final['SO'], gender_distribution)"
      ],
      "metadata": {
        "id": "QJBfgKPgbVCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the min and max year in the 'YEAR NACIMIENTO' column\n",
        "min_year = int(rn_dfs_final['SO']['YEAR NACIMIENTO'].min())\n",
        "max_year = int(rn_dfs_final['SO']['YEAR NACIMIENTO'].max())\n",
        "\n",
        "# Function to generate random years with the specified distribution\n",
        "def generate_random_years(size):\n",
        "    # 75% of the years under 2004\n",
        "    under_2004 = np.random.randint(min_year, 2004, size=int(size * 0.75))\n",
        "    # 25% of the years between 2004 and max_year\n",
        "    over_2004 = np.random.randint(2004, max_year + 1, size=size - int(size * 0.75))\n",
        "    return np.concatenate([under_2004, over_2004])\n",
        "\n",
        "# Function to insert random years into the 'YEAR NACIMIENTO' column where it is empty\n",
        "def insert_random_years(df):\n",
        "    # Find the indices where 'YEAR NACIMIENTO' is empty\n",
        "    empty_indices = df[df['YEAR NACIMIENTO'].isna()].index\n",
        "\n",
        "    # Generate random years based on the specified distribution\n",
        "    random_years = generate_random_years(len(empty_indices))\n",
        "    np.random.shuffle(random_years)  # Shuffle to mix the values\n",
        "\n",
        "    # Assign these random years to the empty 'YEAR NACIMIENTO' rows\n",
        "    df.loc[empty_indices, 'YEAR NACIMIENTO'] = random_years\n",
        "    return df\n",
        "\n",
        "# Apply the function to the 'SO' dataframe\n",
        "rn_dfs_final['SO'] = insert_random_years(rn_dfs_final['SO'])"
      ],
      "metadata": {
        "id": "qm7bSdbPbuo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column named 'SERVICIO' with the name of the dataframe to each dataframe in rn_dfs_final\n",
        "def add_servicio_column(dfs):\n",
        "    for name, df in dfs.items():\n",
        "        df['SERVICIO'] = name\n",
        "    return dfs\n",
        "\n",
        "# Apply the function to rn_dfs_final\n",
        "rn_dfs_final = add_servicio_column(rn_dfs_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PybnAC9reCDb",
        "outputId": "6a52c3fd-e2fa-45c5-d594-1c7b1cbb4926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-113-c5260d67a514>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['SERVICIO'] = name\n",
            "<ipython-input-113-c5260d67a514>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['SERVICIO'] = name\n",
            "<ipython-input-113-c5260d67a514>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['SERVICIO'] = name\n",
            "<ipython-input-113-c5260d67a514>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['SERVICIO'] = name\n",
            "<ipython-input-113-c5260d67a514>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['SERVICIO'] = name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all dataframes into a single dataframe\n",
        "combined_df = pd.concat(rn_dfs_final.values(), ignore_index=True)\n",
        "\n",
        "# Shuffle the rows of the combined dataframe\n",
        "shuffled_combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "shuffled_combined_df['SERVICIO'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_JtWWq1j-Fp",
        "outputId": "62ecfdff-a423-4b40-fc37-8573f451e5e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SERVICIO\n",
              "SO      1457\n",
              "SAAF     567\n",
              "EM       203\n",
              "LOG      182\n",
              "AT       143\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the frequency of each element in the 'SERVICIO' column\n",
        "service_frequency = df['SERVICIO'].value_counts()\n",
        "\n",
        "# Group by 'PROVINCIA' and 'SERVICIO' and count the occurrences\n",
        "service_province_freq = shuffled_combined_df.groupby(['PROVINCIA', 'SERVICIO']).size().reset_index(name='COUNT')\n",
        "\n",
        "# Function to assign services based on frequency\n",
        "def assign_service(row, freq_df):\n",
        "    province = row['PROVINCIA']\n",
        "    possible_services = freq_df[freq_df['PROVINCIA'] == province]\n",
        "    if possible_services.empty:\n",
        "        return np.nan\n",
        "    else:\n",
        "        return np.random.choice(possible_services['SERVICIO'].values, p=possible_services['COUNT'].values / possible_services['COUNT'].sum())\n",
        "\n",
        "# Apply the function to assign services\n",
        "df_8000rows['SERVICIO'] = df_8000rows.apply(assign_service, axis=1, args=(service_province_freq,))\n",
        "\n",
        "# Ensure balanced classes by sampling\n",
        "min_class_size = service_frequency.min()\n",
        "balanced_df = df_8000rows.groupby('SERVICIO').apply(lambda x: x.sample(n=min_class_size, replace=True)).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "1uB0r6aSmkZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the frequency of each service\n",
        "service_frequency = shuffled_combined_df['SERVICIO'].value_counts()\n",
        "\n",
        "# Set the target number of records per service\n",
        "target_size = 1457\n",
        "\n",
        "# Create a balanced dataframe starting with the initial shuffled data\n",
        "to_use_df = shuffled_combined_df.copy()\n",
        "\n",
        "# Loop through each service to balance the dataframe\n",
        "for service, count in service_frequency.items():\n",
        "    if count < target_size:\n",
        "        # Get the rows for the current service\n",
        "        service_df = balanced_df[balanced_df['SERVICIO'] == service]\n",
        "        # Sample additional rows with replacement to reach the target size\n",
        "        additional_samples = service_df.sample(n=target_size - count, replace=True)\n",
        "        to_use_df = pd.concat([to_use_df, additional_samples])\n",
        "\n",
        "# Shuffle the final balanced dataframe\n",
        "to_use_df = to_use_df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "NriTgA3RnJXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the rows of the dataframe ready to use\n",
        "to_use_df = to_use_df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "pTr3ars6w5BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the min and max year in the 'YEAR NACIMIENTO' column\n",
        "min_year = int(to_use_df['YEAR NACIMIENTO'].min())\n",
        "max_year = int(to_use_df['YEAR NACIMIENTO'].max())\n",
        "\n",
        "# Function to generate random years with the specified distribution\n",
        "def generate_random_years(size):\n",
        "    return np.random.randint(min_year, max_year + 1, size=int(size))\n",
        "\n",
        "# Function to insert random years into the 'YEAR NACIMIENTO' column where it is empty\n",
        "def insert_random_years(df):\n",
        "    # Find the indices where 'YEAR NACIMIENTO' is empty\n",
        "    empty_indices = df[df['YEAR NACIMIENTO'].isna()].index\n",
        "\n",
        "    # Generate random years based on the specified distribution\n",
        "    random_years = generate_random_years(len(empty_indices))\n",
        "\n",
        "    # Assign these random years to the empty 'YEAR NACIMIENTO' rows\n",
        "    df.loc[empty_indices, 'YEAR NACIMIENTO'] = random_years\n",
        "    return df\n",
        "\n",
        "# Apply the function to the dataframe\n",
        "to_use_df = insert_random_years(to_use_df)"
      ],
      "metadata": {
        "id": "syZ8IdqFxPc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una función para categorizar por grupos de edad\n",
        "def categorize_age(year):\n",
        "    if year < 1940:\n",
        "        return 'Antes de 1940'\n",
        "    elif year < 1960:\n",
        "        return '1940-1959'\n",
        "    elif year < 1980:\n",
        "        return '1960-1979'\n",
        "    elif year < 2000:\n",
        "        return '1980-1999'\n",
        "    else:\n",
        "        return '2000 en adelante'\n",
        "\n",
        "# Aplicar la función a la columna 'YEAR NACIMIENTO'\n",
        "to_use_df['YEAR NACIMIENTO'] = to_use_df['YEAR NACIMIENTO'].apply(categorize_age)\n",
        "\n",
        "# Convertir las categorías en variables dummy (one-hot encoding)\n",
        "to_use_df = pd.get_dummies(to_use_df, columns=['YEAR NACIMIENTO'])\n",
        "\n",
        "# Ver el resultado\n",
        "print(to_use_df.head())"
      ],
      "metadata": {
        "id": "XU2JPoSa4vau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determina la cantidad de servicios únicos\n",
        "unique_services = to_use_df['SERVICIO'].unique()\n",
        "num_services = len(unique_services)\n",
        "\n",
        "# Determina cuántos registros por servicio necesitas para el conjunto de prueba\n",
        "test_size_per_service = 85 // num_services\n",
        "\n",
        "# Si hay más de 85 registros, ajusta la muestra para que esté balanceada\n",
        "if test_size_per_service * num_services < 85:\n",
        "    additional_samples = 85 - (test_size_per_service * num_services)\n",
        "else:\n",
        "    additional_samples = 0\n",
        "\n",
        "# Crear el conjunto de prueba balanceado\n",
        "test_df = pd.DataFrame()\n",
        "\n",
        "for service in unique_services:\n",
        "    service_df = to_use_df[to_use_df['SERVICIO'] == service]\n",
        "    test_samples = service_df.sample(n=test_size_per_service, replace=False)\n",
        "    test_df = pd.concat([test_df, test_samples])\n",
        "\n",
        "# Añadir muestras adicionales si es necesario\n",
        "if additional_samples > 0:\n",
        "    additional_df = to_use_df[~to_use_df.index.isin(test_df.index)]\n",
        "    additional_samples_df = additional_df.sample(n=additional_samples, replace=False)\n",
        "    test_df = pd.concat([test_df, additional_samples_df])\n",
        "\n",
        "# Eliminar las filas seleccionadas del dataframe original para crear el conjunto de entrenamiento/validación\n",
        "to_use_df = to_use_df.drop(test_df.index)\n",
        "\n",
        "# Verifica el conjunto de prueba\n",
        "print(test_df['SERVICIO'].value_counts())\n",
        "print(test_df.shape)\n",
        "\n",
        "# Verifica el conjunto de entrenamiento/validación\n",
        "print(to_use_df.shape)"
      ],
      "metadata": {
        "id": "3QdShOyt2c_6",
        "outputId": "c888d2ad-b455-406e-ec9a-e1caf29ac176",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SERVICIO\n",
            "EM      17\n",
            "SAAF    17\n",
            "SO      17\n",
            "AT      17\n",
            "LOG     17\n",
            "Name: count, dtype: int64\n",
            "(85, 5)\n",
            "(7200, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Red Neuronal Predictiva\n",
        "\n",
        "Creación de una red neuronal predictiva que sea capaz de clasificar nuevos usuarios con buena precisión en servicio y provincia."
      ],
      "metadata": {
        "id": "2dYYkD8OoRlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HdSgywBv2mHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}