{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ana-AlonsoCanizares/AA_GRUPO3/blob/main/Limpieza_SAAF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Limpieza datos SAAF\n"
      ],
      "metadata": {
        "id": "cncOKnCG9KT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Descarga y limpeza de datos"
      ],
      "metadata": {
        "id": "MtzkVZOatiwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos cargando los datos de la carpeta de Google Drive en la que estén guardados (será necesario cambiar esta ruta en función de la ruta del ordenador).\n",
        "\n",
        "Es necesario aceptar la conexión con la cuenta de Google Drive.\n",
        "\n",
        "Importante y esencial, antes de tirar el código es necesario que la carpeta de Drive a la que se va a llamar tenga los archivos en formato xlsx así nombrados: 'CU_SAAF.xlsx', 'CU_AT.xlsx', 'CU_EMPLEO.xlsx', 'CU_LOG.xlsx' para que no haya errores a la hora de ejecutar este código.\n",
        "\n",
        "**¡OJO! Primeros archivo con extensión xlsx y los de las zonas a clasificar, con extensión csv, en teoría, los csv no deberían modificarse (a menos que las normativas cambien y con ello las zonas de impacto se modifiquen).**\n",
        "\n",
        "**Y todos las columnas deben tener los nombres definidos en el documento \"Resumen de datos por servicio FASPAS\" y en mayúsculas antes de ser subidos al programa.**"
      ],
      "metadata": {
        "id": "SHATfRyI9VF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhwbvGlQ9JVW",
        "outputId": "df087aca-45e0-4525-fa07-cbc26631bec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se importan las librerías necesarias para la parte de carga de datos y su limpieza."
      ],
      "metadata": {
        "id": "T6gtnbKM-e5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cdv4QrHLrle",
        "outputId": "aaefaba8-485d-4a5b-c50f-bc8aa42fc797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "e4n3PNFZ-oIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "folder_path = '/content/drive/My Drive/PFG_FASPAS/SAAF'\n",
        "docs_xlsx = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
        "print(docs_xlsx)\n",
        "\n",
        "# # Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "# folder_path_csv = '/content/drive/My Drive/PFG_FASPAS/Zonas_Prioritarias'\n",
        "# docs_csv = [f for f in os.listdir(folder_path_csv) if f.endswith('.csv')]\n",
        "# print(docs_csv)"
      ],
      "metadata": {
        "id": "yMug4vBB9ozZ",
        "outputId": "ac70d78e-0da0-4203-fdda-bd3e7652cd14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TO_SAAF.xlsx', 'CU_SAAF.xlsx', 'AB_SAAF.xlsx', 'GU_SAAF.xlsx', 'CR_SAAF.xlsx']\n",
            "['municipios_prioritarios_clm.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de un diccionario que almacene los dataframes (.xlsx) contenidos en la carpeta de Cuenca.\n",
        "\n",
        "Se irán cargando todos los archivos en un dataframe que se añade al diccionario con el nombre de dicho archivo como clave."
      ],
      "metadata": {
        "id": "hxl_Z9P1-wXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic_dataframes = {}\n",
        "dic_zonas = {}\n",
        "\n",
        "for doc in docs_xlsx:\n",
        "    entire_path = os.path.join(folder_path, doc)\n",
        "    df = pd.read_excel(entire_path)\n",
        "    # Uso el nombre del archivo como clave\n",
        "    dic_dataframes[doc] = df\n",
        "\n",
        "# for doc in docs_csv:\n",
        "#   entire_path = os.path.join(folder_path_csv, doc)\n",
        "#   df = pd.read_csv(entire_path)\n",
        "#   # Uso el nombre del archivo como clave\n",
        "#   dic_zonas[doc] = df"
      ],
      "metadata": {
        "id": "wpsYAD2X-c4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se debe observar que el dataframe cuente con al menos las columnas: 'AÑOS ATENCIÓN', 'FECHA NACIMIENTO' **en formato fecha**, 'LOCALIDAD', 'GÉNERO' y 'CP'"
      ],
      "metadata": {
        "id": "swAIR8SeSRpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que sabemos cuáles son las claves, guardamos cada dataframe por separado para trabajar con todos ellos de manera individual, por el momento."
      ],
      "metadata": {
        "id": "j7NG1ZuiB2pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir todas las claves\n",
        "for clave in dic_dataframes.keys():\n",
        "    print(clave)"
      ],
      "metadata": {
        "id": "-bSrp6NdAwQZ",
        "outputId": "46441d94-f897-4399-dc06-5fb53ebedde9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TO_SAAF.xlsx\n",
            "CU_SAAF.xlsx\n",
            "AB_SAAF.xlsx\n",
            "GU_SAAF.xlsx\n",
            "CR_SAAF.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a limpiar cada uno de los dataframes en función de los datos que tenemos y vamos a normalizarlos para cuando los crucemos entre ellos.\n",
        "\n",
        "Comenzamos con los datos de Servicio de Atención y Apoyo a Familias (SAAF)."
      ],
      "metadata": {
        "id": "51k6fU9qClCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sacamos la información del dataset y observamos que de 77 entradas, el campo de 'GRADO DEPENDENCIA' tan solo tiene 1 registro, por lo que se procede a eliminarla. El 'GRADO DISCAPACIDAD' de momento lo dejamos ya que para el impacto territorial no lo vamos a usar."
      ],
      "metadata": {
        "id": "TmNPUhvqDIUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # CÓDIGO SOLO PARA EL CURRO (PQ NO PUEDO ACCEDER AL DRIVE)\n",
        "# CU_SAAF = pd.read_excel('/content/CU_SAAF.xlsx')\n",
        "# TO_SAAF = pd.read_excel('/content/TO_SAAF.xlsx')\n",
        "# AB_SAAF = pd.read_excel('/content/AB_SAAF.xlsx')\n",
        "# GU_SAAF = pd.read_excel('/content/GU_SAAF.xlsx')\n",
        "# CR_SAAF = pd.read_excel('/content/CR_SAAF.xlsx')"
      ],
      "metadata": {
        "id": "FzS8Balj4ozG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se ponen todos los nombres de las columnas y todos los valores de los registros en mayúsculas y se eliminan todas las tildes contenidas (tanto en los registros de tipo texto como en las columnas), para uniformar el dataset."
      ],
      "metadata": {
        "id": "V0sIxMvj5RAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función que realiza las transformaciones\n",
        "def transform_df(df):\n",
        "    col_excluded = ['FECHA NACIMIENTO', 'FECHA DE NACIMIENTO', 'FECHA NAC']\n",
        "\n",
        "    # Convertir todas las columnas de tipo object a mayúsculas, excepto las especificadas\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object' and column not in col_excluded:\n",
        "            df[column] = df[column].apply(lambda x: unidecode(x.upper()) if isinstance(x, str) else x)\n",
        "\n",
        "    # Ahora, aplicamos unidecode a los nombres de las columnas\n",
        "    df.columns = [unidecode(col.upper()) for col in df.columns]\n",
        "\n",
        "    return df\n",
        "\n",
        "# Asegurarse de que todas las columnas necesarias existan en todos los dataframes, creándolas si es necesario (OJO: COLUMNAS NECESARIAS PARA TODOS LOS SERVICIOS)\n",
        "# Limpieza de todas aquellas columnas no pedidas\n",
        "def kept_columns(df):\n",
        "  # Lista de columnas necesarias para hacer el impacto territorial en todos los dataframes\n",
        "  all_needed_columns = ['FECHA NACIMIENTO', 'LOCALIDAD', 'GENERO', 'CP']\n",
        "  for col in all_needed_columns:\n",
        "    if col not in df.columns:\n",
        "      df[col] = np.nan\n",
        "\n",
        "  columns_to_keep = ['FECHA ALTA','FECHA DE ALTA', 'FECHA NACIMIENTO','FECHA DE NACIMIENTO','FECHA NAC' ,'GENERO', 'TIPO USUARIO','LOCALIDAD', 'CP', 'PROVINCIA','TIPO SORDERA', 'MOMENTO APARICION', '?LLEVA AUDIFONO?', '?LLEVA I.C.?', 'AUDIFONO','?LLEVA IMPLANTE DE TRONCO CEREBRAL?', '?LLEVA IMPLANTE OSTEOINTEGRADO?', '?LLEVA IMPLANTE DE OIDO MEDIO?', 'MODALIDAD COMUNICATIVA', 'MODALIDAD COMUNICATIVA EN CASA','MOD COM','GRADO DISCAPACIDAD', 'TIPO PROTESIS', 'TIPO DE PROTESIS','TIPO DE PROTESIS AUDITIVA', 'GRADO PERDIDA', 'GRADO DE PERDIDA']\n",
        "  df = df[df.columns.intersection(columns_to_keep)]\n",
        "  return df\n",
        "\n",
        "def mapping_names(df):\n",
        "    # Diccionario con los mapeos deseados\n",
        "    columns_map = {\n",
        "        'FECHA DE ALTA': 'FECHA ALTA',\n",
        "        'FECHA NAC': 'FECHA NACIMIENTO',\n",
        "        'FECHA DE NACIMIENTO': 'FECHA NACIMIENTO',\n",
        "        'MOMENTO DE APARICION DE LA SORDERA': 'MOMENTO APARICION',\n",
        "        'TIPO DE PROTESIS AUDITIVA': 'TIPO PROTESIS',\n",
        "        'TIPO DE PROTESIS': 'TIPO PROTESIS',\n",
        "        'TIPO DE SORDERA': 'TIPO SORDERA',\n",
        "        'TIPO DE USUARIO DEL SAAF': 'TIPO USUARIO',\n",
        "        'MOD COM': 'MODALIDAD COMUNICATIVA',\n",
        "        'MODALIDAD COMUNICATIVA EN CASA': 'MODALIDAD COMUNICATIVA',\n",
        "        'GRADO DE PERDIDA': 'GRADO PERDIDA'\n",
        "    }\n",
        "\n",
        "    # Crear un nuevo diccionario para los nombres de columnas\n",
        "    rename_columns = {}\n",
        "\n",
        "    # Iterar sobre las columnas y aplicar el mapeo\n",
        "    for col in df.columns:\n",
        "        norm_col = columns_map.get(col, col)\n",
        "        rename_columns[col] = norm_col\n",
        "\n",
        "    # Renombrar las columnas del DataFrame\n",
        "    df.rename(columns=rename_columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "def CP_year(df):\n",
        "\n",
        "  df['FECHA NACIMIENTO'] = df['FECHA NACIMIENTO'].apply(lambda x: np.nan if str(x).isdigit() or x == '(ADULTO)' else x)\n",
        "  # df['FECHA NACIMIENTO'] = pd.to_datetime(df['FECHA NACIMIENTO'], errors='coerce')\n",
        "\n",
        "  df['CP'] =df['CP'].astype('Int64')\n",
        "\n",
        "  df['YEAR NACIMIENTO'] = df['FECHA NACIMIENTO'].dt.year\n",
        "\n",
        "  df = df.dropna(subset=['CP', 'LOCALIDAD'], how='all')\n",
        "\n",
        "  CP_loc_filter = df['CP'].isnull() & df['LOCALIDAD'].isnull()\n",
        "\n",
        "  df = df[~CP_loc_filter]\n",
        "  return df\n",
        "\n",
        "def delete_rows(df):\n",
        "  df = df.dropna(subset=['CP', 'LOCALIDAD'], how='all')\n",
        "  return df"
      ],
      "metadata": {
        "id": "1IJU09ZlVrml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a comenzar a limpiar los datos del Servicio de Asistencia y Apoyo a Familias (sAAF)."
      ],
      "metadata": {
        "id": "p64AZybRx3kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo primero que hacemos es eliminar aquellas columnas cuyos datos NO se hayan pedido, ya que aunque no se hayan podido recopilar todos los datos pedidos de todas las provincias, queremos que los datasets sean lo más homogéneos posibles."
      ],
      "metadata": {
        "id": "0Ly23V5wG1nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por otra parte, la columna 'FECHA NACIMIENTO' nos quedamos solo con el año, eliminando la hora. Por defecto, Excel pone el CP como float, asi que lo pasamos a entero."
      ],
      "metadata": {
        "id": "1mM-6P2bFsHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si existe en el dataframe, como en este caso, aquellos que no tienen registro de 'TIPO PRÓTESIS' hay que examinar el motivo. Vamos a ver aquellos registros distintos que tienen todas las columnas. Vemos que en la columna 'TIPO PRÓTESIS' no hay ninguna salida que sea \"NO TIENE\" y preguntamos a FASPAS, tras su aprobación, procedemos a sustituir los valores nulos por 'NO TIENE'."
      ],
      "metadata": {
        "id": "Qo75GIDwHlyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa que la 'MODALIDAD COMUNICATIVA' no da ninguna información, pues todos los registros son \"ORAL\", sim embargo, la dejamos por si acaso en un futuro los registros cambiaran."
      ],
      "metadata": {
        "id": "4-jR--TTKzM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se disminuye la complejidad de los datos de manera que solo nos quedamos con la categoría de 'AUD O PRÓTESIS' que indica si lleva o no alguno de los dispotivos posibles (¿Lleva implante de tronco cerebral/osteointegrado/de oído medio?) con independencia de cuál de estos sea."
      ],
      "metadata": {
        "id": "f0bDOcSLVe57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de columna 'AUD O PRÓTESIS' en función del dataset de entrada\n",
        "def determinar_si_o_no(row):\n",
        "    # Lista de las columnas a verificar\n",
        "    columns_to_meet = ['?LLEVA I.C.?', '?LLEVA IMPLANTE DE TRONCO CEREBRAL?', '?LLEVA IMPLANTE OSTEOINTEGRADO?',\n",
        "                       '?LLEVA IMPLANTE DE OIDO MEDIO?', 'AUDIFONO']\n",
        "    # Itera sobre cada columna especificada\n",
        "    for col in columns_to_meet:\n",
        "        # Verifica si el valor es NaN, \"No\" o \"no tiene\"\n",
        "        if pd.isna(row[col]) or row[col] == 'NO' or row[col] == 'NO TIENE':\n",
        "            return 'NO'\n",
        "    # Si ninguna columna cumple la condición anterior, devuelve 'Sí'\n",
        "    return 'SI'\n",
        "\n",
        "def other_columns(df):\n",
        "  if 'TIPO PROTESIS' in df.columns:\n",
        "    df['TIPO PROTESIS'] = df['TIPO PROTESIS'].fillna('NO TIENE')\n",
        "    df['AUD O PROTESIS'] = np.where(df['TIPO PROTESIS'] == 'NO TIENE', 'NO', 'SI')\n",
        "    df.drop('TIPO PROTESIS', axis=1, inplace=True)\n",
        "\n",
        "  # elif ('¿LLEVA AUDIFONO?' and '¿LLEVA I.C.?' and '¿LLEVA IMPLANTE DE TRONCO CEREBRAL?' and '¿LLEVA IMPLANTE OSTEOINTEGRADO?' and '¿LLEVA IMPLANTE DE OIDO MEDIO?') in df.columns:\n",
        "  elif '?LLEVA I.C.?' in df.columns:\n",
        "    for row in df:\n",
        "      df['AUD O PROTESIS'] = df.apply(determinar_si_o_no, axis=1)\n",
        "\n",
        "  # Creación de columna 'AUD O PRÓTESIS' y elimino las demás\n",
        "  columns_to_drop =['?LLEVA AUDIFONO?' ,'?LLEVA I.C.?', '?LLEVA IMPLANTE DE TRONCO CEREBRAL?',\n",
        "                    '?LLEVA IMPLANTE OSTEOINTEGRADO?', '?LLEVA IMPLANTE DE OIDO MEDIO?']\n",
        "  df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
        "\n",
        "  if 'GENERO' in df.columns:\n",
        "    mujer = ['MUJER', 'HEMBRA', 'FEMENINO', 'FEM', 'F']\n",
        "    hombre = ['HOMBRE', 'MACHO', 'MASCULINO', 'MASC', 'MAS']\n",
        "\n",
        "    for m in mujer:\n",
        "      df['GENERO'] = df['GENERO'].replace(m, 'M')\n",
        "\n",
        "    for h in hombre:\n",
        "      df['GENERO'] = df['GENERO'].replace(h, 'H')\n",
        "  return df"
      ],
      "metadata": {
        "id": "iFVuG1LXH-Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para contemplar que otros datos que se ingresen sean distintos, vamos a jugar con eliminar aquellas columnas que no sean esenciales para el estudio territorial, sino que nos den información más sociológica y sirvan para un estudio social; siempre y cuando el porcentaje de valores nulos sea mayor o igual que 1/3 del total de registros."
      ],
      "metadata": {
        "id": "oiLN3k37MBz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**OJOOO AÑO O ANO DE ATENCIÓN, REVISAR**"
      ],
      "metadata": {
        "id": "8qSa4-bpAbe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_not_important_columns(df):\n",
        "  # Añado 'AÑO ATENCIÓN' para SAAF\n",
        "  needed_columns = ['AÑO ATENCION', 'FECHA NACIMIENTO', 'LOCALIDAD', 'CP', 'YEAR NACIMIENTO']\n",
        "\n",
        "  # Creación de la lista de columnas consideradas para posible eliminación\n",
        "  social_columns = [col for col in df.columns if col not in needed_columns]\n",
        "\n",
        "  # Porcentaje máximo de valores nulos permitido\n",
        "  max_percentage = 1/3\n",
        "\n",
        "  # Identifico columnas para eliminar\n",
        "  del_columns = []\n",
        "  for col in social_columns:\n",
        "      if df[col].isnull().sum() / len(df) >= max_percentage:\n",
        "          del_columns.append(col)\n",
        "\n",
        "  # Eliminar las columnas identificadas\n",
        "  df.drop(columns=del_columns, inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "eGbzUeKaMtVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además, pondremos todos los registros de MUJER como M y HOMBRE como H. Esto lo realizaremos con todos los dataframes. Contemplamos que los registros que llegan pueden tener otros nombres en este campo."
      ],
      "metadata": {
        "id": "acwjNFXXIUOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def left_columns_norm(df):\n",
        "  if 'TIPO SORDERA' in df.columns:\n",
        "    # Lista con las posibles salidas permitidas\n",
        "    strings_to_keep = ['NEUROSENSORIAL', 'CONDUCTIVA', 'MIXTA', 'TEL']\n",
        "\n",
        "    # Sustituyo los registros distintos por la palabra 'NO REGISTRADO' (VER SI SE CAMBIA ESTO)\n",
        "    df.loc[~df['TIPO SORDERA'].isin(strings_to_keep), 'TIPO SORDERA'] = 'NO REGISTRADO'\n",
        "\n",
        "  if 'MOMENTO APARICION' in df.columns:\n",
        "    strings_to_keep = ['PERILOCUTIVA', 'POSTLOCUTIVA', 'PRELOCUTIVA']\n",
        "\n",
        "    # Sustituyo los registros distintos por la palabra 'NO REGISTRADO' (VER SI SE CAMBIA ESTO)\n",
        "    df.loc[~df['MOMENTO APARICION'].isin(strings_to_keep), 'MOMENTO APARICION'] = 'NO REGISTRADO'\n",
        "\n",
        "  if 'MODALIDAD COMUNICATIVA' in df.columns:\n",
        "    strings_to_keep = ['SIGNO', 'LSE','SIGNOS', 'ORAL', 'BIMODAL', 'L.S.E.', 'L.S.E', 'SIGNOS NATURALES',\n",
        "                       'ORAL,SIGNOS NATURALES', 'ORAL, SIGNOS NATURALES']\n",
        "\n",
        "    # Sustituyo los registros distintos por la palabra 'NO REGISTRADO' (VER SI SE CAMBIA ESTO)\n",
        "    df.loc[~df['MODALIDAD COMUNICATIVA'].isin(strings_to_keep), 'MODALIDAD COMUNICATIVA'] = 'NO REGISTRADO'\n",
        "\n",
        "    # Normalizo todas las formas de llamar a las distintas modalidades comunicativas\n",
        "    ORAL = ['ORAL', 'ORAL,SIGNOS NATURALES', 'ORAL, SIGNOS NATURALES']\n",
        "    LSE = ['LSE', 'L.S.E.','L.S.E']\n",
        "    SIGNOS = ['SIGNO', 'SIGNOS', 'SIGNOS NATURALES']\n",
        "\n",
        "    for o in ORAL:\n",
        "      df['MODALIDAD COMUNICATIVA'] = df['MODALIDAD COMUNICATIVA'].replace(o, 'ORAL')\n",
        "\n",
        "    for l in LSE:\n",
        "      df['MODALIDAD COMUNICATIVA'] = df['MODALIDAD COMUNICATIVA'].replace(l, 'LSE')\n",
        "\n",
        "    for s in SIGNOS:\n",
        "      df['MODALIDAD COMUNICATIVA'] = df['MODALIDAD COMUNICATIVA'].replace(s, 'SIGNOS')\n",
        "\n",
        "  if 'GRADO PERDIDA' in df.columns:\n",
        "    df['GRADO PERDIDA'] = df['GRADO PERDIDA'].astype(str)\n",
        "\n",
        "    # Selecciono solo las 3 primeras letras (quitando la parte de DB si la tuviera)\n",
        "    df['GRADO PERDIDA'] = df['GRADO PERDIDA'].str.slice(0, 3)\n",
        "\n",
        "    strings_to_keep = ['DAP', 'DAM', 'DAS', 'DAL']\n",
        "\n",
        "    df.loc[~df['GRADO PERDIDA'].isin(strings_to_keep), 'GRADO PERDIDA'] = np.nan\n",
        "\n",
        "  if 'GRADO DISCAPACIDAD' in df.columns:\n",
        "    df['GRADO DISCAPACIDAD'] = df['GRADO DISCAPACIDAD'].apply(lambda x: x / 100 if x > 1 else x)\n",
        "    df['GRADO DISCAPACIDAD'] = df['GRADO DISCAPACIDAD'].fillna(0)\n",
        "  return df"
      ],
      "metadata": {
        "id": "5F-V0DwZaopA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí podemos ver la información del df, cuáles son las columnas que se tendrán en cuenta para este caso concreto y los diferentes datos que pueden tomar. Observamos que los únicos datos que pueden tener valores nulos son CP y LOCALIDAD, pero serán en registros distintos."
      ],
      "metadata": {
        "id": "BoAn1uH0T7Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea un filtro booleano que coge aquellos registros con ambos campos nulos y se lo aplicamos al dataframe. Ya que aquellos registros sin geolocalización no nos sirven para el objetivo principal que es saber el impacto que se tiene en las diferentes zonas para desarrollar un plan de ampliación territorial estratégico basado en el estudio estadístico."
      ],
      "metadata": {
        "id": "I52DZXN9QMN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que 'dict_of_dfs' es tu diccionario de DataFrames\n",
        "for df_key, df in dic_dataframes.items():\n",
        "    dic_dataframes[df_key] = transform_df(df)\n",
        "    dic_dataframes[df_key] = mapping_names(df)\n",
        "    dic_dataframes[df_key] = kept_columns(df)\n",
        "    dic_dataframes[df_key] = CP_year(df)\n",
        "    dic_dataframes[df_key] = other_columns(df)\n",
        "    dic_dataframes[df_key] = delete_rows(df)"
      ],
      "metadata": {
        "id": "zPGQTBgp9hiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CU_SAAF = dic_dataframes.get(\"CU_SAAF.xlsx\")\n",
        "TO_SAAF = dic_dataframes.get(\"TO_SAAF.xlsx\")\n",
        "AB_SAAF = dic_dataframes.get(\"AB_SAAF.xlsx\")\n",
        "GU_SAAF = dic_dataframes.get(\"GU_SAAF.xlsx\")\n",
        "CR_SAAF = dic_dataframes.get(\"CR_SAAF.xlsx\")"
      ],
      "metadata": {
        "id": "tgagsBVtCDkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # CÓDIGO DE USO PARA EL CURRO\n",
        "# CU_SAAF = transform_df(CU_SAAF)\n",
        "# CU_SAAF = mapping_names(CU_SAAF)\n",
        "# CU_SAAF = kept_columns(CU_SAAF)\n",
        "# CU_SAAF = CP_year(CU_SAAF)\n",
        "# CU_SAAF = other_columns(CU_SAAF)\n",
        "# CU_SAAF = delete_rows(CU_SAAF)"
      ],
      "metadata": {
        "id": "d_ZQ7fPK-AL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # CÓDIGO DE USO PARA EL CURRO\n",
        "# GU_SAAF = transform_df(GU_SAAF)\n",
        "# GU_SAAF = mapping_names(GU_SAAF)\n",
        "# GU_SAAF = kept_columns(GU_SAAF)\n",
        "# GU_SAAF = CP_year(GU_SAAF)\n",
        "# GU_SAAF = other_columns(GU_SAAF)\n",
        "# GU_SAAF = delete_rows(GU_SAAF)"
      ],
      "metadata": {
        "id": "KpcNdfft05D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # CÓDIGO DE USO PARA EL CURRO\n",
        "# TO_SAAF = transform_df(TO_SAAF)\n",
        "# TO_SAAF = mapping_names(TO_SAAF)\n",
        "# TO_SAAF = kept_columns(TO_SAAF)\n",
        "# TO_SAAF = CP_year(TO_SAAF)\n",
        "# TO_SAAF = other_columns(TO_SAAF)\n",
        "# TO_SAAF = delete_rows(TO_SAAF)"
      ],
      "metadata": {
        "id": "I0XNnELq1Cos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # CÓDIGO DE USO PARA EL CURRO\n",
        "# AB_SAAF = transform_df(AB_SAAF)\n",
        "# AB_SAAF = mapping_names(AB_SAAF)\n",
        "# AB_SAAF = kept_columns(AB_SAAF)\n",
        "# AB_SAAF = CP_year(AB_SAAF)\n",
        "# AB_SAAF = other_columns(AB_SAAF)\n",
        "# AB_SAAF = left_columns_norm(AB_SAAF)\n",
        "# AB_SAAF = delete_rows(AB_SAAF)"
      ],
      "metadata": {
        "id": "YsLvgEbK1KYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # CÓDIGO DE USO PARA EL CURRO\n",
        "# CR_SAAF = transform_df(CR_SAAF)\n",
        "# CR_SAAF = mapping_names(CR_SAAF)\n",
        "# CR_SAAF = kept_columns(CR_SAAF)\n",
        "# CR_SAAF = CP_year(CR_SAAF)\n",
        "# CR_SAAF = other_columns(CR_SAAF)"
      ],
      "metadata": {
        "id": "OSMAV73e1NBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLM_SAAF = pd.concat([CU_SAAF, CR_SAAF, GU_SAAF, TO_SAAF, AB_SAAF])"
      ],
      "metadata": {
        "id": "AUuiz7aeZJJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLM_SAAF = delete_not_important_columns(CLM_SAAF)\n",
        "CLM_SAAF = left_columns_norm(CLM_SAAF)\n",
        "CLM_SAAF.info()"
      ],
      "metadata": {
        "id": "X2X0HUmtdkHv",
        "outputId": "4a6583cd-4579-4eda-d9e6-9535637bde6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 767 entries, 0 to 211\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count  Dtype         \n",
            "---  ------            --------------  -----         \n",
            " 0   FECHA NACIMIENTO  767 non-null    datetime64[ns]\n",
            " 1   LOCALIDAD         636 non-null    object        \n",
            " 2   CP                545 non-null    Int64         \n",
            " 3   TIPO SORDERA      767 non-null    object        \n",
            " 4   YEAR NACIMIENTO   767 non-null    int32         \n",
            " 5   AUD O PROTESIS    541 non-null    object        \n",
            " 6   FECHA ALTA        550 non-null    datetime64[ns]\n",
            "dtypes: Int64(1), datetime64[ns](2), int32(1), object(3)\n",
            "memory usage: 45.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ESTO LO HAGO DESPUÉS DE SACAR EL DOC PARA CLM ENTERO, ASÍ NO QUITO NINGÚN DATO PARA EL INFORME DE COMUNIDAD AUTÓNOMA\n",
        "AB_SAAF = delete_not_important_columns(AB_SAAF)\n",
        "AB_SAAF = left_columns_norm(AB_SAAF)\n",
        "\n",
        "CU_SAAF = delete_not_important_columns(CU_SAAF)\n",
        "CU_SAAF = left_columns_norm(CU_SAAF)\n",
        "\n",
        "CR_SAAF = delete_not_important_columns(CR_SAAF)\n",
        "CR_SAAF = left_columns_norm(CR_SAAF)\n",
        "\n",
        "GU_SAAF = delete_not_important_columns(GU_SAAF)\n",
        "GU_SAAF = left_columns_norm(GU_SAAF)\n",
        "\n",
        "TO_SAAF = delete_not_important_columns(TO_SAAF)\n",
        "TO_SAAF = left_columns_norm(TO_SAAF)"
      ],
      "metadata": {
        "id": "M46HKGt7ZLDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Salvaguardar los archivos\n",
        "Se guardan los archivos en la carpeta de las provincias correspondientes y en la de CLM."
      ],
      "metadata": {
        "id": "0UzCsgNI0sR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of DataFrames\n",
        "dfs = [AB_SAAF, CR_SAAF, CU_SAAF, GU_SAAF, TO_SAAF, CLM_SAAF]\n",
        "\n",
        "# Corresponding folder paths on Google Drive\n",
        "folder_paths = [\n",
        "    '/content/drive/My Drive/PFG_FASPAS/AB',\n",
        "    '/content/drive/My Drive/PFG_FASPAS/CR',\n",
        "    '/content/drive/My Drive/PFG_FASPAS/CU',\n",
        "    '/content/drive/My Drive/PFG_FASPAS/GU',\n",
        "    '/content/drive/My Drive/PFG_FASPAS/TO',\n",
        "    '/content/drive/My Drive/PFG_FASPAS/CLM'\n",
        "]\n",
        "\n",
        "# Corresponding file names\n",
        "file_names = ['AB_SAAF_limpio.xlsx', 'CR_SAAF_limpio.xlsx', 'CU_SAAF_limpio.xlsx', 'GU_SAAF_limpio.xlsx', 'TO_SAAF_limpio.xlsx', 'CLM_SAAF_limpio.xlsx']"
      ],
      "metadata": {
        "id": "ZellXLwa6DuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the DataFrames, folder paths, and file names\n",
        "for df, folder_path, file_name in zip(dfs, folder_paths, file_names):\n",
        "    # Define the complete file path\n",
        "    file_path = f\"{folder_path}/{file_name}\"\n",
        "\n",
        "    # Save the DataFrame as an Excel file\n",
        "    df.to_excel(file_path, index=False)\n",
        "\n",
        "print(\"All DataFrames have been exported successfully.\")"
      ],
      "metadata": {
        "id": "yCu891sF6tmq",
        "outputId": "6c328f3e-dcee-454f-f5bb-005dbc65be51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All DataFrames have been exported successfully.\n"
          ]
        }
      ]
    }
  ]
}