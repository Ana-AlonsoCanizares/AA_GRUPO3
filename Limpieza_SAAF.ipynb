{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ana-AlonsoCanizares/AA_GRUPO3/blob/main/Limpieza_SAAF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Informe Provincial Cuenca\n",
        "##Descarga y limpeza de datos"
      ],
      "metadata": {
        "id": "cncOKnCG9KT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos cargando los datos de la carpeta de Google Drive en la que estén guardados (será necesario cambiar esta ruta en función de la ruta del ordenador).\n",
        "\n",
        "Es necesario aceptar la conexión con la cuenta de Google Drive.\n",
        "\n",
        "Importante y esencial, antes de tirar el código es necesario que la carpeta de Drive a la que se va a llamar tenga los archivos en formato xlsx así nombrados: 'CU_SAAF.xlsx', 'CU_AT.xlsx', 'CU_EMPLEO.xlsx', 'CU_LOG.xlsx' para que no haya errores a la hora de ejecutar este código.\n",
        "\n",
        "**¡OJO! Primeros archivo con extensión xlsx y los de las zonas a clasificar, con extensión csv, en teoría, los csv no deberían modificarse (a menos que las normativas cambien y con ello las zonas de impacto se modifiquen).**\n",
        "\n",
        "**Y todos las columnas deben tener los nombres definidos en el documento \"Resumen de datos por servicio FASPAS\" y en mayúsculas antes de ser subidos al programa.**"
      ],
      "metadata": {
        "id": "SHATfRyI9VF6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "mhwbvGlQ9JVW",
        "outputId": "0bb27c84-1950-4455-8a68-ee7daa4342d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The domain policy has disabled Drive File Stream: https://support.google.com/a/answer/7496409",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    286\u001b[0m       \u001b[0;31m# Now kill bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_signal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    289\u001b[0m           \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_disabled_drivefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m           \u001b[0;34m+\u001b[0m \u001b[0;34m': https://support.google.com/a/answer/7496409'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The domain policy has disabled Drive File Stream: https://support.google.com/a/answer/7496409"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se importan las librerías necesarias para la parte de carga de datos y su limpieza."
      ],
      "metadata": {
        "id": "T6gtnbKM-e5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cdv4QrHLrle",
        "outputId": "0098dc70-cd46-467e-83d3-f503e59825e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "e4n3PNFZ-oIq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "folder_path = '/content/drive/My Drive/PFG_FASPAS/SAAF'\n",
        "docs_xlsx = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
        "print(docs_xlsx)\n",
        "\n",
        "# Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "folder_path_csv = '/content/drive/My Drive/PFG_FASPAS/Zonas_Prioritarias'\n",
        "docs_csv = [f for f in os.listdir(folder_path_csv) if f.endswith('.csv')]\n",
        "print(docs_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "yMug4vBB9ozZ",
        "outputId": "56eedb47-abbf-4d18-bbb1-23331f6ebb1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/PFG_FASPAS/SAAF'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c01e17291611>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/PFG_FASPAS/SAAF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdocs_xlsx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_xlsx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/PFG_FASPAS/SAAF'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de un diccionario que almacene los dataframes (.xlsx) contenidos en la carpeta de Cuenca.\n",
        "\n",
        "Se irán cargando todos los archivos en un dataframe que se añade al diccionario con el nombre de dicho archivo como clave."
      ],
      "metadata": {
        "id": "hxl_Z9P1-wXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic_dataframes = {}\n",
        "dic_zonas = {}\n",
        "\n",
        "for doc in docs_xlsx:\n",
        "    entire_path = os.path.join(folder_path, doc)\n",
        "    df = pd.read_excel(entire_path)\n",
        "    # Uso el nombre del archivo como clave\n",
        "    dic_dataframes[doc] = df\n",
        "\n",
        "for doc in docs_csv:\n",
        "  entire_path = os.path.join(folder_path_csv, doc)\n",
        "  df = pd.read_csv(entire_path)\n",
        "  # Uso el nombre del archivo como clave\n",
        "  dic_zonas[doc] = df"
      ],
      "metadata": {
        "id": "wpsYAD2X-c4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se debe observar que el dataframe cuente con al menos las columnas: 'AÑOS ATENCIÓN', 'FECHA NACIMIENTO' **en formato fecha**, 'LOCALIDAD', 'GÉNERO' y 'CP'"
      ],
      "metadata": {
        "id": "swAIR8SeSRpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que sabemos cuáles son las claves, guardamos cada dataframe por separado para trabajar con todos ellos de manera individual, por el momento."
      ],
      "metadata": {
        "id": "j7NG1ZuiB2pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Imprimir todas las claves\n",
        "# for clave in dic_dataframes.keys():\n",
        "#     print(clave)"
      ],
      "metadata": {
        "id": "-bSrp6NdAwQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CU_SAAF = dic_dataframes.get(\"CU_SAAF.xlsx\")\n",
        "# CU_AT = dic_dataframes.get(\"CU_AT.xlsx\")\n",
        "# CU_EMPLEO = dic_dataframes.get(\"CU_EMPLEO.xlsx\")\n",
        "# CU_LOG = dic_dataframes.get(\"CU_LOG.xlsx\")\n",
        "# # CU_SOCIOS = dic_dataframes.get(\"CU_SOCIOS.xlsx\")"
      ],
      "metadata": {
        "id": "tgagsBVtCDkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a limpiar cada uno de los dataframes en función de los datos que tenemos y vamos a normalizarlos para cuando los crucemos entre ellos.\n",
        "\n",
        "Comenzamos con los datos de Servicio de Atención y Apoyo a Familias (SAAF)."
      ],
      "metadata": {
        "id": "51k6fU9qClCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sacamos la información del dataset y observamos que de 77 entradas, el campo de 'GRADO DEPENDENCIA' tan solo tiene 1 registro, por lo que se procede a eliminarla. El 'GRADO DISCAPACIDAD' de momento lo dejamos ya que para el impacto territorial no lo vamos a usar."
      ],
      "metadata": {
        "id": "TmNPUhvqDIUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # CÓDIGO SOLO PARA EL CURRO (PQ NO PUEDO ACCEDER AL DRIVE)\n",
        "CU_SAAF = pd.read_excel('/content/CU_SAAF_PRUEBA.xlsx')\n",
        "# CU_AT = pd.read_excel('/content/CU_AT.xlsx')\n",
        "# CU_EMPLEO = pd.read_excel('/content/CU_EMPLEO.xlsx')\n",
        "# CU_LOG = pd.read_excel('/content/CU_LOG.xlsx')\n",
        "# # CU_SOCIOS = pd.read_excel('/content/CU_SOCIOS.xlsx')\n",
        "mun_pri_clm = pd.read_csv('/content/municipios_prioritarios_clm.csv')"
      ],
      "metadata": {
        "id": "FzS8Balj4ozG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se ponen todos los nombres de las columnas y todos los valores de los registros en mayúsculas y se eliminan todas las tildes contenidas (tanto en los registros de tipo texto como en las columnas), para uniformar el dataset."
      ],
      "metadata": {
        "id": "V0sIxMvj5RAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función que realiza las transformaciones\n",
        "def transform_df(df):\n",
        "    # Convertir todas las cadenas en las columnas de objeto a mayúsculas\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object':\n",
        "            df[column] = df[column].str.upper()\n",
        "\n",
        "    # Convertir los nombres de las columnas a mayúsculas y sin tildes\n",
        "    df.columns = [unidecode(col.upper()) for col in df.columns]\n",
        "\n",
        "    # Seleccionar las columnas de texto\n",
        "    text_columns = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Aplicar unidecode solo a las columnas de texto\n",
        "    df[text_columns] = df[text_columns].applymap(lambda x: unidecode(x) if isinstance(x, str) else x)\n",
        "\n",
        "    # Devolver el DataFrame transformado\n",
        "    return df\n",
        "\n",
        "# Asegurarse de que todas las columnas necesarias existan en todos los dataframes, creándolas si es necesario (OJO: COLUMNAS NECESARIAS PARA TODOS LOS SERVICIOS)\n",
        "# Limpieza de todas aquellas columnas no pedidas\n",
        "def kept_columns(df):\n",
        "  # Lista de columnas necesarias para hacer el impacto territorial en todos los dataframes\n",
        "  all_needed_columns = ['FECHA NACIMIENTO', 'LOCALIDAD', 'GENERO', 'CP']\n",
        "  for col in all_needed_columns:\n",
        "    if col not in df.columns:\n",
        "      df[col] = np.nan\n",
        "\n",
        "  columns_to_keep = ['FECHA ALTA','FECHA DE ALTA', 'FECHA NACIMIENTO','FECHA DE NACIMIENTO','FECHA NAC' ,'LOCALIDAD', 'GENERO', 'CP', 'TIPO SORDERA', 'MOMENTO APARICION', 'MOMENTO DE APARICION DE LA SORDERA', '¿LLEVA AUDIFONO?', '¿LLEVA I.C.?', '¿LLEVA IMPLANTE DE TRONCO CEREBRAL?', '¿LLEVA IMPLANTE OSTEOINTEGRADO?', '¿LLEVA IMPLANTE DE OIDO MEDIO?', 'MODALIDAD COMUNICATIVA', 'MODALIDAD COMUNICATIVA EN CASA','MOD COM','GRADO DISCAPACIDAD', 'TIPO PROTESIS', 'TIPO DE PROTESIS','TIPO DE PROTESIS AUDITIVA', 'GRADO PERDIDA', 'GRADO DE PERDIDA']\n",
        "  df = df[df.columns.intersection(columns_to_keep)]\n",
        "  return df\n",
        "\n",
        "def mapping_names(df):\n",
        "    # Diccionario con los mapeos deseados\n",
        "    columns_map = {\n",
        "        'FECHA DE ALTA': 'FECHA ALTA',\n",
        "        'FECHA NAC': 'FECHA NACIMIENTO',\n",
        "        'FECHA DE NACIMIENTO': 'FECHA NACIMIENTO',\n",
        "        'MOMENTO DE APARICION DE LA SORDERA': 'MOMENTO APARICION',\n",
        "        'TIPO DE PROTESIS AUDITIVA': 'TIPO PROTESIS',\n",
        "        'TIPO DE PROTESIS': 'TIPO PROTESIS',\n",
        "        'MOD COM': 'MODALIDAD COMUNICATIVA',\n",
        "        'MODALIDAD COMUNICATIVA EN CASA': 'MODALIDAD COMUNICATIVA',\n",
        "        'GRADO DE PERDIDA': 'GRADO PERDIDA'\n",
        "    }\n",
        "\n",
        "    # Crear un nuevo diccionario para los nombres de columnas\n",
        "    rename_columns = {}\n",
        "\n",
        "    # Iterar sobre las columnas y aplicar el mapeo\n",
        "    for col in df.columns:\n",
        "        norm_col = columns_map.get(col, col)\n",
        "        rename_columns[col] = norm_col\n",
        "\n",
        "    # Renombrar las columnas del DataFrame\n",
        "    df.rename(columns=rename_columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "def CP_year(df):\n",
        "\n",
        "  # df['FECHA NACIMIENTO'] = df['FECHA NACIMIENTO'].apply(lambda x: np.nan if str(x).isdigit() or x == '(ADULTO)' else x)\n",
        "  # df['FECHA NACIMIENTO'] = pd.to_datetime(df['FECHA NACIMIENTO'], errors='coerce')\n",
        "\n",
        "  df['CP'] =df['CP'].astype('Int64')\n",
        "\n",
        "  df['YEAR NACIMIENTO'] = df['FECHA NACIMIENTO'].dt.year\n",
        "\n",
        "  df = df.dropna(subset=['CP', 'LOCALIDAD'], how='all')\n",
        "\n",
        "  CP_loc_filter = df['CP'].isnull() & df['LOCALIDAD'].isnull()\n",
        "\n",
        "  df = df[~CP_loc_filter]\n",
        "  return df"
      ],
      "metadata": {
        "id": "1IJU09ZlVrml"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OJOOOOOOOO HAY QUE VER CÓMO PROCEDER PARA AQUELLOS REGISTROS CON DATOS NULOS EN LOS CAMPOS DE CP Y LOCALIDAD**\n",
        "\n",
        "¿¿SI HAY UNO DE LOS DOS DEJAMOS EL REGISTRO, PERO SI NO HAY NADA EN AMBOS CAMPOS LOS ELIMINAMOS?? está justo arriba"
      ],
      "metadata": {
        "id": "rIr8fpZ38qbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a comenzar a limpiar los datos del Servicio de Asistencia y Apoyo a Familias (sAAF)."
      ],
      "metadata": {
        "id": "p64AZybRx3kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo primero que hacemos es eliminar aquellas columnas cuyos datos NO se hayan pedido, ya que aunque no se hayan podido recopilar todos los datos pedidos de todas las provincias, queremos que los datasets sean lo más homogéneos posibles."
      ],
      "metadata": {
        "id": "0Ly23V5wG1nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por otra parte, la columna 'FECHA NACIMIENTO' nos quedamos solo con la fecha (OJOOOO ¿O SOLO EL AÑO???? O AMBAS OPCIONES), eliminando la hora. Por defecto, Excel pone el CP como float, asi que lo pasamos a entero.\n",
        "\n",
        "**YO NO QUITARÍA LA FECHA DE NACIMIENTO CONCRETA POR SI HAY QUE CONTEMPLAR QUE LOS DATOS DE ENTRADA TENGAN USUARIOS REPETIDOS, PERO PARA HACER ESTUDIO DE EDAD, USARÍA LA COLUMNA DE 'AÑO NACIMIENTO'. CONSULTAAAAAAAAAAAAAAAR.**"
      ],
      "metadata": {
        "id": "1mM-6P2bFsHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si existe en el dataframe, como en este caso, aquellos que no tienen registro de 'TIPO PRÓTESIS' hay que examinar el motivo. Vamos a ver aquellos registros distintos que tienen todas las columnas. Vemos que en la columna 'TIPO PRÓTESIS' no hay ninguna salida que sea \"NO TIENE\" y preguntamos a FASPAS, tras su aprobación, procedemos a sustituir los valores nulos por 'NO TIENE'."
      ],
      "metadata": {
        "id": "Qo75GIDwHlyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa que la 'MODALIDAD COMUNICATIVA' no da ninguna información, pues todos los registros son \"ORAL\", sim embargo, la dejamos por si acaso en un futuro los registros cambiaran."
      ],
      "metadata": {
        "id": "4-jR--TTKzM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se disminuye la complejidad de los datos de manera que solo nos quedamos con la categoría de 'AUD O PRÓTESIS' que indica si lleva o no alguno de los dispotivos posibles (¿Lleva implante de tronco cerebral/osteointegrado/de oído medio?) con independencia de cuál de estos sea."
      ],
      "metadata": {
        "id": "f0bDOcSLVe57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de columna 'AUD O PRÓTESIS' en función del dataset de entrada\n",
        "def determinar_si_o_no(row):\n",
        "    # Lista de las columnas a verificar\n",
        "    columns_to_meet = ['¿LLEVA AUDIFONO?', '¿LLEVA I.C.?', '¿LLEVA IMPLANTE DE TRONCO CEREBRAL?', '¿LLEVA IMPLANTE OSTEOINTEGRADO?', '¿LLEVA IMPLANTE DE OIDO MEDIO?']\n",
        "    # Itera sobre cada columna especificada\n",
        "    for col in columns_to_meet:\n",
        "        # Verifica si el valor es NaN, \"No\" o \"no tiene\"\n",
        "        if pd.isna(row[col]) or row[col] == 'NO' or row[col] == 'NO TIENE':\n",
        "            return 'NO'\n",
        "    # Si ninguna columna cumple la condición anterior, devuelve 'Sí'\n",
        "    return 'SI'\n",
        "\n",
        "def other_columns(df):\n",
        "  if 'TIPO PROTESIS' in df.columns:\n",
        "    df['TIPO PROTESIS'] = df['TIPO PROTESIS'].fillna('NO TIENE')\n",
        "    df['AUD O PROTESIS'] = np.where(df['TIPO PROTESIS'] == 'NO TIENE', 'NO', 'SI')\n",
        "    df.drop('TIPO PROTESIS', axis=1, inplace=True)\n",
        "\n",
        "  elif ('¿LLEVA AUDIFONO?' and '¿LLEVA I.C.?' and '¿LLEVA IMPLANTE DE TRONCO CEREBRAL?' and '¿LLEVA IMPLANTE OSTEOINTEGRADO?' and '¿LLEVA IMPLANTE DE OIDO MEDIO?') in df.columns:\n",
        "    df['AUD O PROTESIS'] = df.apply(determinar_si_o_no, axis=1)\n",
        "\n",
        "  # if '¿LLEVA AUDIFONO?' in df.columns:\n",
        "  #   si = ['SI', 'YES', 'S']\n",
        "  #   for s in si:\n",
        "  #     df['¿LLEVA AUDIFONO?'] = np.where(df['¿LLEVA AUDIFONO?'] == s, 'SI', 'NO')\n",
        "  #     df['¿LLEVA I.C.?'] = np.where(df['¿LLEVA I.C.?'] == s, 'SI', 'NO')\n",
        "  #     df['¿LLEVA IMPLANTE DE TRONCO CEREBRAL?'] = np.where(df['¿LLEVA IMPLANTE DE TRONCO CEREBRAL?'] == s, 'SI', 'NO')\n",
        "  #     df['¿LLEVA IMPLANTE OSTEOINTEGRADO?'] = np.where(df['¿LLEVA IMPLANTE OSTEOINTEGRADO?'] == s, 'SI', 'NO')\n",
        "  #     df['¿LLEVA IMPLANTE DE OIDO MEDIO?'] = np.where(df['¿LLEVA IMPLANTE DE OIDO?'] == s, 'SI', 'NO')\n",
        "\n",
        "  #   # Creación de columna 'AUD O PRÓTESIS' y elimino las demás\n",
        "  #   df['AUD O PROTESIS'] = df[['¿LLEVA AUDIFONO?' ,'¿LLEVA I.C.?', '¿LLEVA IMPLANTE DE TRONCO CEREBRAL?', '¿LLEVA IMPLANTE OSTEOINTEGRADO?', '¿LLEVA IMPLANTE DE OIDO MEDIO?']].apply(lambda x: x == 'SI').any(axis=1)\n",
        "    df.drop(columns=['¿LLEVA AUDIFONO?' ,'¿LLEVA I.C.?', '¿LLEVA IMPLANTE DE TRONCO CEREBRAL?', '¿LLEVA IMPLANTE OSTEOINTEGRADO?', '¿LLEVA IMPLANTE DE OIDO MEDIO?'], inplace=True)\n",
        "\n",
        "  if 'GENERO' in df.columns:\n",
        "    # OJO: contemplar tb las diferentes formas de escribir TODAS LAS COLUMNAS 'MOMENTO APARICIÓN', HAY QUE NORMALIZAR TODO\n",
        "    mujer = ['MUJER', 'HEMBRA', 'FEMENINO', 'FEM', 'F']\n",
        "    hombre = ['HOMBRE', 'MACHO', 'MASCULINO', 'MASC', 'MAS']\n",
        "\n",
        "    for m in mujer:\n",
        "      df['GENERO'] = df['GENERO'].replace(m, 'M')\n",
        "\n",
        "    for h in hombre:\n",
        "      df['GENERO'] = df['GENERO'].replace(h, 'H')\n",
        "  return df"
      ],
      "metadata": {
        "id": "iFVuG1LXH-Fp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para contemplar que otros datos que se ingresen sean distintos, vamos a jugar con eliminar aquellas columnas que no sean esenciales para el estudio territorial, sino que nos den información más sociológica y sirvan para un estudio social; siempre y cuando el porcentaje de valores nulos sea mayor o igual que 1/3 del total de registros."
      ],
      "metadata": {
        "id": "oiLN3k37MBz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_not_important_columns(df):\n",
        "  # Añado 'AÑO ATENCIÓN' para SAAF\n",
        "  needed_columns = ['AÑO ATENCION', 'FECHA NACIMIENTO', 'LOCALIDAD', 'GENERO', 'CP', 'YEAR NACIMIENTO']\n",
        "\n",
        "  # Creación de la lista de columnas consideradas para posible eliminación\n",
        "  social_columns = [col for col in df.columns if col not in needed_columns]\n",
        "\n",
        "  # Porcentaje máximo de valores nulos permitido\n",
        "  max_percentage = 1/3\n",
        "\n",
        "  # Identifico columnas para eliminar\n",
        "  del_columns = []\n",
        "  for col in social_columns:\n",
        "      if df[col].isnull().sum() / len(df) >= max_percentage:\n",
        "          del_columns.append(col)\n",
        "\n",
        "  # Eliminar las columnas identificadas\n",
        "  df.drop(columns=del_columns, inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "eGbzUeKaMtVx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además, pondremos todos los registros de MUJER como M y HOMBRE como H. Esto lo realizaremos con todos los dataframes. Contemplamos que los registros que llegan pueden tener otros nombres en este campo."
      ],
      "metadata": {
        "id": "acwjNFXXIUOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def left_columns_norm(df):\n",
        "  if 'TIPO SORDERA' in df.columns:\n",
        "    # Lista con las posibles salidas permitidas\n",
        "    strings_to_keep = ['NEUROSENSORIAL', 'CONDUCTIVA', 'MIXTA', 'TEL']\n",
        "\n",
        "    # Sustituyo los registros distintos por la palabra 'NO REGISTRADO' (VER SI SE CAMBIA ESTO)\n",
        "    df.loc[~df['TIPO SORDERA'].isin(strings_to_keep), 'TIPO SORDERA'] = 'NO REGISTRADO'\n",
        "\n",
        "  if 'MOMENTO APARICION' in df.columns:\n",
        "    strings_to_keep = ['PERILOCUTIVA', 'POSTLOCUTIVA', 'PRELOCUTIVA']\n",
        "\n",
        "    # Sustituyo los registros distintos por la palabra 'NO REGISTRADO' (VER SI SE CAMBIA ESTO)\n",
        "    df.loc[~df['MOMENTO APARICION'].isin(strings_to_keep), 'MOMENTO APARICION'] = 'NO REGISTRADO'\n",
        "\n",
        "  if 'MODALIDAD COMUNICATIVA' in df.columns:\n",
        "    strings_to_keep = ['SIGNO', 'LSE','SIGNOS', 'ORAL', 'BIMODAL', 'L.S.E.', 'L.S.E', 'SIGNOS NATURALES', 'ORAL,SIGNOS NATURALES', 'ORAL, SIGNOS NATURALES']\n",
        "\n",
        "    # Sustituyo los registros distintos por la palabra 'NO REGISTRADO' (VER SI SE CAMBIA ESTO)\n",
        "    df.loc[~df['MODALIDAD COMUNICATIVA'].isin(strings_to_keep), 'MODALIDAD COMUNICATIVA'] = 'NO REGISTRADO'\n",
        "\n",
        "    # Normalizo todas las formas de llamar a las distintas modalidades comunicativas\n",
        "    ORAL = ['ORAL', 'ORAL,SIGNOS NATURALES', 'ORAL, SIGNOS NATURALES']\n",
        "    LSE = ['LSE', 'L.S.E.','L.S.E']\n",
        "    SIGNOS = ['SIGNO', 'SIGNOS', 'SIGNOS NATURALES']\n",
        "\n",
        "    for o in ORAL:\n",
        "      df['MODALIDAD COMUNICATIVA'] = df['MODALIDAD COMUNICATIVA'].replace(o, 'ORAL')\n",
        "\n",
        "    for l in LSE:\n",
        "      df['MODALIDAD COMUNICATIVA'] = df['MODALIDAD COMUNICATIVA'].replace(l, 'LSE')\n",
        "\n",
        "    for s in SIGNOS:\n",
        "      df['MODALIDAD COMUNICATIVA'] = df['MODALIDAD COMUNICATIVA'].replace(s, 'SIGNOS')\n",
        "\n",
        "  if 'GRADO PERDIDA' in df.columns:\n",
        "    df['GRADO PERDIDA'] = df['GRADO PERDIDA'].astype(str)\n",
        "\n",
        "    # Selecciono solo las 3 primeras letras (quitando la parte de DB si la tuviera)\n",
        "    df['GRADO PERDIDA'] = df['GRADO PERDIDA'].str.slice(0, 3)\n",
        "\n",
        "    strings_to_keep = ['DAP', 'DAM', 'DAS', 'DAL']\n",
        "\n",
        "    df.loc[~df['GRADO PERDIDA'].isin(strings_to_keep), 'GRADO PERDIDA'] = 'NO REGISTRADO'\n",
        "\n",
        "  if 'GRADO DISCAPACIDAD' in df.columns:\n",
        "    df['GRADO DISCAPACIDAD'] = df['GRADO DISCAPACIDAD'].apply(lambda x: x / 100 if x > 1 else x)\n",
        "    # OJOOOOOOOOOOOOOOO PREGUNTAR A OLGA SI LE PARECE BIEN O MEJOR PONERLO A CERO LAS QUE NO TENGAN REGISTRO!?!?\n",
        "    df['GRADO DISCAPACIDAD'] = df['GRADO DISCAPACIDAD'].fillna(0)\n",
        "    # df['GRADO DISCAPACIDAD'] = df['GRADO DISCAPACIDAD'].fillna(df['GRADO DISCAPACIDAD'].mean())\n",
        "  return df"
      ],
      "metadata": {
        "id": "5F-V0DwZaopA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí podemos ver la información del df, cuáles son las columnas que se tendrán en cuenta para este caso concreto y los diferentes datos que pueden tomar. Observamos que los únicos datos que pueden tener valores nulos son CP y LOCALIDAD, pero serán en registros distintos."
      ],
      "metadata": {
        "id": "BoAn1uH0T7Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea un filtro booleano que coge aquellos registros con ambos campos nulos y se lo aplicamos al dataframe. Ya que aquellos registros sin geolocalización no nos sirven para el objetivo principal que es saber el impacto que se tiene en las diferentes zonas para desarrollar un plan de ampliación territorial estratégico basado en el estudio estadístico."
      ],
      "metadata": {
        "id": "I52DZXN9QMN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que 'dict_of_dfs' es tu diccionario de DataFrames\n",
        "for df_key, df in dic_dataframes.items():\n",
        "    dic_dataframes[df_key] = transform_df(df)\n",
        "    dic_dataframes[df_key] = mapping_names(df)\n",
        "    dic_dataframes[df_key] = kept_columns(df)\n",
        "    dic_dataframes[df_key] = CP_year(df)\n",
        "    dic_dataframes[df_key] = other_columns(df)\n",
        "    dic_dataframes[df_key] = delete_not_important_columns(df)\n",
        "    dic_dataframes[df_key] = left_columns_norm(df)"
      ],
      "metadata": {
        "id": "zPGQTBgp9hiF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÓDIGO DE USO PARA EL CURRO\n",
        "CU_SAAF = transform_df(CU_SAAF)\n",
        "CU_SAAF = mapping_names(CU_SAAF)\n",
        "CU_SAAF = kept_columns(CU_SAAF)\n",
        "CU_SAAF = CP_year(CU_SAAF)\n",
        "CU_SAAF = other_columns(CU_SAAF)\n",
        "CU_SAAF = delete_not_important_columns(CU_SAAF)\n",
        "CU_SAAF = left_columns_norm(CU_SAAF)"
      ],
      "metadata": {
        "id": "d_ZQ7fPK-AL2",
        "outputId": "80f0e890-9a31-417c-b04b-f44b150abf02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "Can only use .dt accessor with datetimelike values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-fddd13d640a9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mCU_SAAF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapping_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCU_SAAF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCU_SAAF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkept_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCU_SAAF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mCU_SAAF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCP_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCU_SAAF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mCU_SAAF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCU_SAAF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mCU_SAAF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelete_not_important_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCU_SAAF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-66fc71c33762>\u001b[0m in \u001b[0;36mCP_year\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'YEAR NACIMIENTO'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FECHA NACIMIENTO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LOCALIDAD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/accessors.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mPeriodProperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .dt accessor with datetimelike values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_dataframes['CU_SAAF.xlsx'].info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "cSeUe27-gdB_",
        "outputId": "166721bb-7cf8-440d-aa81-363247d33ee1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dic_dataframes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-09dc7600ed6b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdic_dataframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CU_SAAF.xlsx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dic_dataframes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÓDIGO DE USO PARA EL CURRO\n",
        "CU_SAAF.info()\n",
        "column_names = CU_SAAF.columns.tolist()\n",
        "\n",
        "for column in column_names:\n",
        "  unique_val = CU_SAAF[column].unique()\n",
        "  print(unique_val)"
      ],
      "metadata": {
        "id": "1QItI6b9_phA",
        "outputId": "75711bc6-440a-441a-eb7b-26c61a059889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 77 entries, 0 to 76\n",
            "Data columns (total 9 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   FECHA NACIMIENTO        4 non-null      object \n",
            " 1   GENERO                  77 non-null     object \n",
            " 2   LOCALIDAD               76 non-null     object \n",
            " 3   CP                      72 non-null     float64\n",
            " 4   TIPO SORDERA            77 non-null     object \n",
            " 5   GRADO PERDIDA           77 non-null     object \n",
            " 6   MOMENTO APARICION       77 non-null     object \n",
            " 7   MODALIDAD COMUNICATIVA  77 non-null     object \n",
            " 8   GRADO DISCAPACIDAD      77 non-null     float64\n",
            "dtypes: float64(2), object(7)\n",
            "memory usage: 5.5+ KB\n",
            "[nan '(ADULTO)']\n",
            "['M' 'H']\n",
            "['NOHALES' 'VALVERDE DEL JUCAR' 'CARBONERAS' 'CUENCA' 'MINGLANILLA'\n",
            " 'POZORRUBIO' 'CHILLARON' 'CAMPILLO DE ALTOBUEY' 'TARANCON'\n",
            " 'VILLAMAYOR DE SANTIAGO' 'VALERA DE ABAJO' 'ATALAYA DE CANAVATE' 'CANETE'\n",
            " 'MOTA DEL CUERVO' 'MOTILLA DEL PALANCAR' 'PRIEGO' nan\n",
            " 'TORRUBIA DEL CAMPO' 'HUETE' 'BELMONTE' 'CANAMARES' 'VILLAR DE OLALLA'\n",
            " 'LEDANA' 'VILLALBA DE LA SIERRA' 'REILLO' 'HORCAJO DE SANTIAGO']\n",
            "[16191. 16100. 16350. 16004. 16003. 16260. 16414. 16190. 16210. 16410.\n",
            " 16415. 16120. 16710. 16300.    nan 16630. 16200. 16002. 16000. 16400.\n",
            " 16001. 16800. 16413. 16500. 16640. 16890. 16195. 16237. 16140. 16390.]\n",
            "['NEUROSENSORIAL' 'MIXTA' 'CONDUCTIVA' 'TEL']\n",
            "['DAP' 'DAS' 'DAM' 'DAL' 'NO REGISTRADO']\n",
            "['PRELOCUTIVA' 'POSTLOCUTIVA' 'PERILOCUTIVA' 'NO REGISTRADO']\n",
            "['ORAL']\n",
            "[0.7  0.41 0.   0.44 0.33 0.46 0.43 0.36 0.63 0.56 0.71 0.53 0.76 0.42\n",
            " 0.35 0.45 0.5  0.3  0.39 0.24 0.8  0.65]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = dic_dataframes['CU_SAAF.xlsx'].columns.tolist()\n",
        "\n",
        "for column in column_names:\n",
        "  unique_val = dic_dataframes['CU_SAAF.xlsx'][column].unique()\n",
        "  print(unique_val)"
      ],
      "metadata": {
        "id": "1doWoxX2Jltq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_comun = pd.merge(dic_dataframes['CU_SAAF.xlsx'], mun_pri_clm, left_on='LOCALIDAD', right_on='ZONA PRIORITARIA')\n",
        "df_comun"
      ],
      "metadata": {
        "id": "i4n9cHTXcCqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos el impacto que tiene en las zonas prioritarias."
      ],
      "metadata": {
        "id": "41R1ETSNgyxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular el conteo de registros para cada categoría\n",
        "cat_count = df_comun['LOCALIDAD'].value_counts()\n",
        "\n",
        "# Crear un gráfico de barras\n",
        "cat_count.sort_index().plot(kind='bar')\n",
        "\n",
        "# Personalización adicional\n",
        "plt.title('SAAF: Impacto en Zonas por Zonas Prioritarias')\n",
        "plt.xlabel('ZONA PRIORITARIA')\n",
        "plt.ylabel('Número de Registros')\n",
        "plt.xticks(rotation=45)  # Rota las etiquetas del eje X para mejor lectura\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yA6sWBBaeR-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"En este caso concreto las localidades con impacto dentro de las que se incluyen en las zonas prioritarias son: \\n\\nLocalidad Nº Registros\")\n",
        "print(cat_count)\n",
        "# DEFINIR QUÉ SE CONSIDERA IMPACTO FUERTE/IMPACTO NORMAL/IMPACTO LEVE\n",
        "# Iterar sobre la serie y mostrar solo las localidades con un valor mayor que 10\n",
        "for localidad, valor in cat_count.items():\n",
        "    if valor > 10:\n",
        "      print(f\"La Localidad de {localidad} tiene un impacto fuerte, ya que cuenta con {valor} registros.\")\n",
        "    elif valor > 5:\n",
        "      print(f\"La Localidad de {localidad} tiene un impacto medio, ya que cuenta con {valor} registros.\")\n",
        "    else:\n",
        "      if valor > 1:\n",
        "        print(f\"La Localidad de {localidad} tiene un impacto leve, ya que cuenta con {valor} registros.\")\n",
        "      else:\n",
        "        print(f\"La Localidad de {localidad} tiene un impacto leve, ya que cuenta con {valor} registro.\")"
      ],
      "metadata": {
        "id": "_jZTIFrahZzL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}