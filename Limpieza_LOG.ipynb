{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Limpieza servicio logopedia\n",
        "#### **NO hay datos de ALBACETE**\n"
      ],
      "metadata": {
        "id": "cnOLRgPUbszf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RpuopuAEb6ta"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLTdD1uocEfl",
        "outputId": "36cf86f3-6b25-4166-d153-7f4772b08e63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rQ0dgAzxb9Dg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "# folder_path = '/content/drive/My Drive/PFG_FASPAS/SAAF'\n",
        "# docs_xlsx = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
        "# print(docs_xlsx)\n",
        "\n",
        "# # Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "# folder_path_csv = '/content/drive/My Drive/PFG_FASPAS/Zonas_Prioritarias'\n",
        "# docs_csv = [f for f in os.listdir(folder_path_csv) if f.endswith('.csv')]\n",
        "# print(docs_csv)"
      ],
      "metadata": {
        "id": "Blt7RvhgcG7d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dic_dataframes = {}\n",
        "# dic_zonas = {}\n",
        "\n",
        "# for doc in docs_xlsx:\n",
        "#     entire_path = os.path.join(folder_path, doc)\n",
        "#     df = pd.read_excel(entire_path)\n",
        "#     # Uso el nombre del archivo como clave\n",
        "#     dic_dataframes[doc] = df\n",
        "\n",
        "# for doc in docs_csv:\n",
        "#   entire_path = os.path.join(folder_path_csv, doc)\n",
        "#   df = pd.read_csv(entire_path)\n",
        "#   # Uso el nombre del archivo como clave\n",
        "#   dic_zonas[doc] = df"
      ],
      "metadata": {
        "id": "ssYdy0LvcIQc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Imprimir todas las claves\n",
        "# for clave in dic_dataframes.keys():\n",
        "#     print(clave)"
      ],
      "metadata": {
        "id": "c_PeZbZecKca"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CU_SAAF = dic_dataframes.get(\"CU_SAAF.xlsx\")\n",
        "# CU_AT = dic_dataframes.get(\"CU_AT.xlsx\")\n",
        "# CU_EMPLEO = dic_dataframes.get(\"CU_EMPLEO.xlsx\")\n",
        "# CU_LOG = dic_dataframes.get(\"CU_LOG.xlsx\")\n",
        "# # CU_SOCIOS = dic_dataframes.get(\"CU_SOCIOS.xlsx\")"
      ],
      "metadata": {
        "id": "Kfti6G8gcLua"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÓDIGO SOLO PARA EL CURRO (PQ NO PUEDO ACCEDER AL DRIVE)\n",
        "CU_LOG = pd.read_excel('/content/CU_LOG.xlsx')\n",
        "TO_LOG = pd.read_excel('/content/TO_LOG.xlsx')\n",
        "# AB_LOG = pd.read_excel('/content/AB_LOG.xlsx')\n",
        "GU_LOG = pd.read_excel('/content/GU_LOG.xlsx')\n",
        "CR_LOG = pd.read_excel('/content/CR_LOG.xlsx')"
      ],
      "metadata": {
        "id": "iCWw6s7zcPdR"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función que realiza las transformaciones\n",
        "def transform_df(df):\n",
        "    col_excluded = ['FECHA NACIMIENTO', 'Fecha nacimiento', 'Inicio Tratamiento', 'FECHA INICIO/FECHA REVISIÓN']\n",
        "\n",
        "    # Convertir todas las columnas de tipo object a mayúsculas, excepto las especificadas\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object' and column not in col_excluded:\n",
        "            df[column] = df[column].apply(lambda x: unidecode(x.upper()) if isinstance(x, str) else x)\n",
        "\n",
        "    # Ahora, aplicamos unidecode a los nombres de las columnas\n",
        "    df.columns = [unidecode(col.upper()) for col in df.columns]\n",
        "\n",
        "    return df\n",
        "\n",
        "def mapping_names(df):\n",
        "    # Diccionario con los mapeos deseados\n",
        "    columns_map = {\n",
        "        'C. POSTAL': 'CP',\n",
        "        'MOMENTO DE APARICION DE LA SORDERA': 'MOMENTO APARICION',\n",
        "        'SIST. COMUNICACION': 'SISTEMA COMUNICACION',\n",
        "        'SISTEMA DE COMUNICACION': 'SISTEMA COMUNICACION',\n",
        "        'TIPO': 'TIPO SORDERA',\n",
        "        'TIPO HIPOACUSIA': 'TIPO SORDERA',\n",
        "        'GRADO': 'GRADO SORDERA',\n",
        "        'TIPO PROTESIS': 'PROTESIS',\n",
        "        'AUDIF/I.C': 'PROTESIS',\n",
        "        'FECHA INICIO/FECHA REVISION': 'INICIO TRATAMIENTO'\n",
        "    }\n",
        "\n",
        "    # Crear un nuevo diccionario para los nombres de columnas\n",
        "    rename_columns = {}\n",
        "\n",
        "    # Iterar sobre las columnas y aplicar el mapeo\n",
        "    for col in df.columns:\n",
        "        norm_col = columns_map.get(col, col)\n",
        "        rename_columns[col] = norm_col\n",
        "\n",
        "    # Renombrar las columnas del DataFrame\n",
        "    df.rename(columns=rename_columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "def CP_year(df):\n",
        "  if not pd.api.types.is_datetime64_any_dtype(df['FECHA NACIMIENTO']):\n",
        "    df['FECHA NACIMIENTO'] = pd.to_datetime(df['FECHA NACIMIENTO'], errors='coerce')\n",
        "\n",
        "  df['FECHA NACIMIENTO'] = df['FECHA NACIMIENTO'].apply(lambda x: np.nan if str(x).isdigit() or x == '(ADULTO)' else x)\n",
        "  # df['FECHA NACIMIENTO'] = pd.to_datetime(df['FECHA NACIMIENTO'], errors='coerce')\n",
        "  if 'CP' in df.columns:\n",
        "    df['CP'] =df['CP'].astype('Int64')\n",
        "\n",
        "    df['YEAR NACIMIENTO'] = df['FECHA NACIMIENTO'].dt.year\n",
        "\n",
        "    df = df.dropna(subset=['CP', 'LOCALIDAD'], how='all')\n",
        "\n",
        "    CP_loc_filter = df['CP'].isnull() & df['LOCALIDAD'].isnull()\n",
        "\n",
        "    df = df[~CP_loc_filter]\n",
        "  else:\n",
        "    df['YEAR NACIMIENTO'] = df['FECHA NACIMIENTO'].dt.year\n",
        "\n",
        "    df = df.dropna(subset=['LOCALIDAD'], how='all')\n",
        "\n",
        "    loc_filter = df['LOCALIDAD'].isnull()\n",
        "\n",
        "    df = df[~loc_filter]\n",
        "  return df\n",
        "\n",
        "def norm_disc(val):\n",
        "    if pd.isna(val):\n",
        "        return val  # Retorna NaN tal como está\n",
        "    if isinstance(val, str):  # Verificar si el valor es una cadena\n",
        "        if val.startswith('SI'):\n",
        "            if '%' in val:\n",
        "                return int(val.split('(')[1].split('%')[0])  # Extraer el número del porcentaje\n",
        "            return 'SI'  # Convertir \"SI\" y variantes sin número a \"SI\"\n",
        "        if val.startswith('NO'):\n",
        "            return 'NO'  # Convertir \"NO TIENE\" y \"NO\" a \"NO\"\n",
        "        if 'PERM' in val:\n",
        "            return int(val.split()[0])  # Extraer el número antes de \"PERMANENTE\"\n",
        "        return val  # Retorna el valor tal cual si no se aplica ninguna condición anterior\n",
        "    return val  # Maneja números directamente si no son cadenas\n",
        "\n",
        "def norm_dep(value):\n",
        "    if pd.isna(value):\n",
        "        return value  # Retorna NaN tal como está\n",
        "    if isinstance(value, str):\n",
        "        value = value.strip()  # Limpiar espacios en blanco\n",
        "        if value.upper() == 'NO' or value.upper() == 'NO TIENE' or value == 'N0':\n",
        "            return 'NO'\n",
        "        elif value.upper() == 'SI':\n",
        "            return 'SI'\n",
        "        elif 'SI TIPO' in value:\n",
        "            return value.split('SI ')[1]  # Extraer solo la parte después de \"SI \"\n",
        "        else:\n",
        "            return value\n",
        "    elif isinstance(value, int):\n",
        "        if value in [1, 2, 3]:\n",
        "            return f'TIPO {value}'\n",
        "    return value\n",
        "\n",
        "def other_columns(df):\n",
        "  if 'PROTESIS' in df.columns:\n",
        "    df['PROTESIS'] = df['PROTESIS'].fillna('NO TIENE')\n",
        "    df['AUD O PROTESIS'] = np.where(df['PROTESIS'] == 'NO TIENE', 'NO', 'SI')\n",
        "    df.drop('PROTESIS', axis=1, inplace=True)\n",
        "\n",
        "  if 'TIPO SORDERA' in df.columns:\n",
        "    mapeo_sordera = {\n",
        "    'NEUROSENSORIAL': 'NEUROSENSORIAL',\n",
        "    'NEUSENSORIAL': 'NEUROSENSORIAL',\n",
        "    'HIPOACUSIA NEUROSENSORIAL': 'NEUROSENSORIAL',\n",
        "    'HIPOACUSIA NEUROSENSORIAL PROFUNDA': 'NEUROSENSORIAL',\n",
        "    'NEURO': 'NEUROSENSORIAL',\n",
        "    'NEURO ??': 'NEUROSENSORIAL',\n",
        "\n",
        "    'CONDUCTIVA': 'CONDUCTIVA',\n",
        "    'HIPOACUSIA DE CONDUCCION': 'CONDUCTIVA',\n",
        "    'HIPOACUSIA CONDUCTIVA UNILATERAL': 'CONDUCTIVA',\n",
        "    'CONDUC': 'CONDUCTIVA',\n",
        "\n",
        "    'MIXTA': 'MIXTA',\n",
        "    'HIPOACUSIA MIXTA': 'MIXTA',\n",
        "    'HIPOACUSIA MIXTA Y NEUROSENSORIAL': 'MIXTA',\n",
        "\n",
        "    'HIPOACUSIA': 'HIPOACUSIA',\n",
        "    'HIPOACUSIA ENDOCOCLEAR': 'HIPOACUSIA',\n",
        "\n",
        "    'DIFICULTADES DEL LENGUAJE': 'DIFICULTADES DEL LENGUAJE',\n",
        "    'RETRASO DEL LENGUAJE': 'RETRASO DEL LENGUAJE',\n",
        "\n",
        "    '-': 'INDEFINIDO',\n",
        "    'nan': 'INDEFINIDO'\n",
        "    }\n",
        "    df['TIPO SORDERA'] = df['TIPO SORDERA'].map(mapeo_sordera).fillna('INDEFINIDO')\n",
        "  if 'GRADO DISCAPACIDAD' in df.columns:\n",
        "    df['GRADO DISCAPACIDAD'] = df['GRADO DISCAPACIDAD'].apply(norm_disc)\n",
        "  if 'LOCALIDAD' in df.columns:\n",
        "    # Limpiar los datos eliminando contenido entre paréntesis\n",
        "    df['LOCALIDAD'] = df['LOCALIDAD'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
        "  if 'MOMENTO APARICION' in df.columns:\n",
        "    mapeo_apar = {\n",
        "        'PRE': 'PRELOCUTIVA',\n",
        "        'POST': 'POSTLOCUTIVA',\n",
        "        'PERI': 'PERILOCUTIVA',\n",
        "        '-': 'INDEFINIDO',\n",
        "        'nan': 'INDEFINIDO'\n",
        "    }\n",
        "    df['MOMENTO APARICION'] = df['MOMENTO APARICION'].map(mapeo_apar).fillna('INDEFINIDO')\n",
        "  if 'GRADO DEPENDENCIA' in df.columns:\n",
        "    df['GRADO DEPENDENCIA'] = df['GRADO DEPENDENCIA'].apply(norm_dep)\n",
        "  return df\n",
        "\n",
        "def year_conversion(df):\n",
        "  df['presente'] = 1\n",
        "\n",
        "  # Crear pivot table\n",
        "  pivot_df = df.pivot_table(index='FECHA NACIMIENTO', columns='ANO ATENCION', values='presente', fill_value=0, aggfunc='max')\n",
        "\n",
        "  # Restablecer el índice para hacer que id_persona sea una columna otra vez\n",
        "  pivot_df.reset_index(inplace=True)\n",
        "\n",
        "  df_original_clean = df.drop_duplicates(subset=['FECHA NACIMIENTO'])\n",
        "  df_final = pd.merge(df_original_clean, pivot_df, on=['FECHA NACIMIENTO'], how='left')\n",
        "\n",
        "  return df_final\n",
        "\n",
        "def convert_year_columns_to_int(df):\n",
        "    # Crear un nuevo diccionario para mapear nombres antiguos a nuevos\n",
        "    new_columns = {}\n",
        "    for col in df.columns:\n",
        "        # Intentar convertir la columna a entero si es posible\n",
        "        try:\n",
        "            # Convertir a entero si el nombre de la columna es numérico\n",
        "            new_columns[col] = int(col)\n",
        "        except ValueError:\n",
        "            # Mantener el nombre original si no es un año\n",
        "            new_columns[col] = col\n",
        "\n",
        "    # Renombrar las columnas usando el diccionario mapeado\n",
        "    df.rename(columns=new_columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "def CU_year_conversion(df):\n",
        "    # Asegurar que todos los valores en 'ANO ATENCION' sean strings\n",
        "    df['ANO ATENCION'] = df['ANO ATENCION'].astype(str)\n",
        "\n",
        "    # Normalizar los datos: eliminar espacios y caracteres extraños\n",
        "    df['ANO ATENCION'] = df['ANO ATENCION'].apply(lambda x: x.replace(' ', '').replace('_', '').split('-'))\n",
        "\n",
        "    # Explode de los años de atención para preparar para get_dummies\n",
        "    df_exploded = df.explode('ANO ATENCION')\n",
        "\n",
        "    # Aplicar get_dummies para convertir años en columnas binarias\n",
        "    df_one_hot = pd.get_dummies(df_exploded['ANO ATENCION'].astype(str))\n",
        "\n",
        "    # Limpiar los nombres de columnas eliminando espacios y caracteres no deseados\n",
        "    df_one_hot.columns = df_one_hot.columns.str.replace('[^0-9]', '', regex=True).str.strip()\n",
        "\n",
        "    # Consolidar columnas duplicadas resultantes de la limpieza (suma)\n",
        "    df_one_hot = df_one_hot.groupby(df_one_hot.columns, axis=1).sum()\n",
        "\n",
        "    # Agregar las otras columnas para hacer un merge posteriormente\n",
        "    df_one_hot = pd.concat([df_exploded[['FECHA NACIMIENTO']], df_one_hot], axis=1)\n",
        "\n",
        "    # Agrupar por FECHA NACIMIENTO y sumar (esto asegura que solo habrá 0s y 1s)\n",
        "    df_final = df_one_hot.groupby(['FECHA NACIMIENTO']).sum().reset_index()\n",
        "\n",
        "    # Merge el DataFrame original con el DataFrame final transformado\n",
        "    df_merged = pd.merge(df, df_final, on=['FECHA NACIMIENTO'], how='left')\n",
        "    df_merged = convert_year_columns_to_int(df_merged)\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "def keep_columns(df):\n",
        "    desired_columns = ['FECHA NACIMIENTO', 'GENERO', 'LOCALIDAD', 'CP', 'PROVINCIA','MOMENTO APARICION','TIPO SORDERA', 'GRADO PERDIDA', 'AUD O PROTESIS', 'GRADO DEPENDENCIA', 'GRADO DISCAPACIDAD', 'SISTEMA DE COMUNICACION',\n",
        "                       'YEAR NACIMIENTO', 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
        "    # Filtrar el DataFrame para incluir solo las columnas que existen en el DataFrame\n",
        "    filtered_df = df[df.columns.intersection(desired_columns)]\n",
        "    return filtered_df"
      ],
      "metadata": {
        "id": "Ic0XNdCMdRsY"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Suponiendo que 'dict_of_dfs' es tu diccionario de DataFrames\n",
        "# for df_key, df in dic_dataframes.items():\n",
        "#     dic_dataframes[df_key] = transform_df(df)\n",
        "#     dic_dataframes[df_key] = mapping_names(df)\n",
        "#     dic_dataframes[df_key] = kept_columns(df)\n",
        "#     dic_dataframes[df_key] = CP_year(df)\n",
        "#     dic_dataframes[df_key] = other_columns(df)\n",
        "#     dic_dataframes[df_key] = delete_not_important_columns(df)\n",
        "#     dic_dataframes[df_key] = left_columns_norm(df)"
      ],
      "metadata": {
        "id": "NA5jA9TkdgdB"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OJO CR NO TIENE CP NI LOCALIDAD, NO PUEDO CRUZAR LOS DATOS CON NADA**"
      ],
      "metadata": {
        "id": "AZ2lBiqOhiOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CÓDIGO DE USO PARA EL CURRO\n",
        "CU_LOG = transform_df(CU_LOG)\n",
        "CU_LOG = mapping_names(CU_LOG)\n",
        "CU_LOG = CP_year(CU_LOG)\n",
        "CU_LOG = other_columns(CU_LOG)\n",
        "CU_LOG = CU_year_conversion(CU_LOG)\n",
        "CU_LOG = keep_columns(CU_LOG)\n",
        "\n",
        "GU_LOG = transform_df(GU_LOG)\n",
        "GU_LOG = mapping_names(GU_LOG)\n",
        "GU_LOG = CP_year(GU_LOG)\n",
        "GU_LOG = other_columns(GU_LOG)\n",
        "GU_LOG = year_conversion(GU_LOG)\n",
        "GU_LOG = keep_columns(GU_LOG)\n",
        "\n",
        "TO_LOG = transform_df(TO_LOG)\n",
        "TO_LOG = mapping_names(TO_LOG)\n",
        "TO_LOG = CP_year(TO_LOG)\n",
        "TO_LOG = other_columns(TO_LOG)\n",
        "TO_LOG = year_conversion(TO_LOG)\n",
        "TO_LOG = keep_columns(TO_LOG)\n",
        "\n",
        "CR_LOG = transform_df(CR_LOG)\n",
        "CR_LOG = mapping_names(CR_LOG)\n",
        "# CR_LOG = CP_year(CR_LOG)\n",
        "CR_LOG = other_columns(CR_LOG)\n",
        "CR_LOG = year_conversion(CR_LOG)\n",
        "CR_LOG = keep_columns(CR_LOG)"
      ],
      "metadata": {
        "id": "_0YtPJ-rdlgX"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR QUÉ APARECEN AÑOS X E Y???**"
      ],
      "metadata": {
        "id": "-Rk3D0T3rHRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLM_LOG = pd.concat([CU_LOG, CR_LOG, GU_LOG, TO_LOG])"
      ],
      "metadata": {
        "id": "sSA3mQWcQM7z"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para bajarse los excel al ordena\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "OehLdg2wtwMB",
        "outputId": "26d39272-555f-417b-fb20-3c283075b672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = r'\\\\windows.ocaso.pri\\UsuariosCentral_01\\AM5M\\My Documents\\Ana Alonso\\PFG\\DATO\\LOG NORM\\CLM_LOG_norm.xlsx'\n",
        "# Exportar el DataFrame a un archivo Excel\n",
        "CLM_LOG.to_excel(ruta, index=False, engine='openpyxl')\n",
        "\n",
        "ruta = r'\\\\windows.ocaso.pri\\UsuariosCentral_01\\AM5M\\My Documents\\Ana Alonso\\PFG\\DATO\\LOG NORM\\CU_LOG_norm.xlsx'\n",
        "CU_LOG.to_excel(ruta, index=False, engine='openpyxl')\n",
        "\n",
        "ruta = r'\\\\windows.ocaso.pri\\UsuariosCentral_01\\AM5M\\My Documents\\Ana Alonso\\PFG\\DATO\\LOG NORM\\GU_LOG_norm.xlsx'\n",
        "GU_LOG.to_excel(ruta, index=False, engine='openpyxl')\n",
        "\n",
        "ruta = r'\\\\windows.ocaso.pri\\UsuariosCentral_01\\AM5M\\My Documents\\Ana Alonso\\PFG\\DATO\\LOG NORM\\TO_LOG_norm.xlsx'\n",
        "TO_LOG.to_excel(ruta, index=False, engine='openpyxl')\n",
        "\n",
        "# ruta = r'\\\\windows.ocaso.pri\\UsuariosCentral_01\\AM5M\\My Documents\\Ana Alonso\\PFG\\DATO\\LOG NORM\\AB_LOG_norm.xlsx'CR_LOG.to_excel(ruta, index=False, engine='openpyxl')\n",
        "# AB_LOG.to_excel(ruta, index=False, engine='openpyxl')"
      ],
      "metadata": {
        "id": "x9uCfUAwt8O0"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "my_documents = Path.home() / 'Documentos'\n",
        "\n",
        "# Ruta completa donde guardar el archivo\n",
        "ruta_archivo = my_documents / 'Ana Alonso' / 'PFG' / 'datos_exportados.xlsx'\n",
        "\n",
        "# Crear directorios si no existen\n",
        "ruta_archivo.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Exportar a Excel\n",
        "CLM_LOG.to_excel(ruta_archivo, index=False, engine='openpyxl')"
      ],
      "metadata": {
        "id": "YxdA027KvS3v"
      },
      "execution_count": 242,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}