{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Limpieza servicio logopedia"
      ],
      "metadata": {
        "id": "cnOLRgPUbszf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RpuopuAEb6ta",
        "outputId": "a83628c1-6d27-4d69-9835-b96b2f79d302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLTdD1uocEfl",
        "outputId": "3b7a06fa-e112-4856-a602-cbd8a072a70c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rQ0dgAzxb9Dg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "folder_path = '/content/drive/My Drive/PFG_FASPAS/LOG'\n",
        "docs_xlsx = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
        "print(docs_xlsx)"
      ],
      "metadata": {
        "id": "Blt7RvhgcG7d",
        "outputId": "77e5877a-f64f-4cbe-edc1-faa65c20d1a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GU_LOG.xlsx', 'TO_LOG.xlsx', 'CR_LOG.xlsx', 'CU_LOG.xlsx']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_dataframes = {}\n",
        "dic_zonas = {}\n",
        "\n",
        "for doc in docs_xlsx:\n",
        "    entire_path = os.path.join(folder_path, doc)\n",
        "    df = pd.read_excel(entire_path)\n",
        "    # Uso el nombre del archivo como clave\n",
        "    dic_dataframes[doc] = df"
      ],
      "metadata": {
        "id": "ssYdy0LvcIQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir todas las claves\n",
        "for clave in dic_dataframes.keys():\n",
        "    print(clave)"
      ],
      "metadata": {
        "id": "c_PeZbZecKca",
        "outputId": "00083aed-3e25-4a82-b4e0-58abc17418ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GU_LOG.xlsx\n",
            "TO_LOG.xlsx\n",
            "CR_LOG.xlsx\n",
            "CU_LOG.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CU_SAAF = dic_dataframes.get(\"CU_SAAF.xlsx\")\n",
        "# CU_AT = dic_dataframes.get(\"CU_AT.xlsx\")\n",
        "# CU_EMPLEO = dic_dataframes.get(\"CU_EMPLEO.xlsx\")\n",
        "# CU_LOG = dic_dataframes.get(\"CU_LOG.xlsx\")\n",
        "# # CU_SOCIOS = dic_dataframes.get(\"CU_SOCIOS.xlsx\")"
      ],
      "metadata": {
        "id": "Kfti6G8gcLua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # CÓDIGO SOLO PARA EL CURRO (PQ NO PUEDO ACCEDER AL DRIVE)\n",
        "CU_LOG = pd.read_excel('/content/CU_LOG.xlsx')\n",
        "TO_LOG = pd.read_excel('/content/TO_LOG.xlsx')\n",
        "# AB_LOG = pd.read_excel('/content/AB_LOG.xlsx')\n",
        "GU_LOG = pd.read_excel('/content/GU_LOG.xlsx')\n",
        "CR_LOG = pd.read_excel('/content/CR_LOG.xlsx')"
      ],
      "metadata": {
        "id": "iCWw6s7zcPdR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función que realiza las transformaciones\n",
        "def transform_df(df):\n",
        "    col_excluded = ['FECHA NACIMIENTO', 'Fecha nacimiento', 'Inicio Tratamiento', 'FECHA INICIO/FECHA REVISIÓN']\n",
        "\n",
        "    # Convertir todas las columnas de tipo object a mayúsculas, excepto las especificadas\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object' and column not in col_excluded:\n",
        "            df[column] = df[column].apply(lambda x: unidecode(x.upper()) if isinstance(x, str) else x)\n",
        "\n",
        "    # Ahora, aplicamos unidecode a los nombres de las columnas\n",
        "    df.columns = [unidecode(col.upper()) for col in df.columns]\n",
        "\n",
        "    return df\n",
        "\n",
        "def mapping_names(df):\n",
        "    # Diccionario con los mapeos deseados\n",
        "    columns_map = {\n",
        "        'C. POSTAL': 'CP',\n",
        "        'MOMENTO DE APARICION DE LA SORDERA': 'MOMENTO APARICION',\n",
        "        'SIST. COMUNICACION': 'SISTEMA COMUNICACION',\n",
        "        'SISTEMA DE COMUNICACION': 'SISTEMA COMUNICACION',\n",
        "        'TIPO': 'TIPO SORDERA',\n",
        "        'TIPO HIPOACUSIA': 'TIPO SORDERA',\n",
        "        'GRADO': 'GRADO SORDERA',\n",
        "        'TIPO PROTESIS': 'PROTESIS',\n",
        "        'AUDIF/I.C': 'PROTESIS',\n",
        "        'FECHA INICIO/FECHA REVISION': 'INICIO TRATAMIENTO'\n",
        "    }\n",
        "\n",
        "    # Crear un nuevo diccionario para los nombres de columnas\n",
        "    rename_columns = {}\n",
        "\n",
        "    # Iterar sobre las columnas y aplicar el mapeo\n",
        "    for col in df.columns:\n",
        "        norm_col = columns_map.get(col, col)\n",
        "        rename_columns[col] = norm_col\n",
        "\n",
        "    # Renombrar las columnas del DataFrame\n",
        "    df.rename(columns=rename_columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "def CP_year(df):\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df['FECHA NACIMIENTO']):\n",
        "        df['FECHA NACIMIENTO'] = pd.to_datetime(df['FECHA NACIMIENTO'], errors='coerce')\n",
        "\n",
        "    df['FECHA NACIMIENTO'] = df['FECHA NACIMIENTO'].apply(lambda x: np.nan if str(x).isdigit() or x == '(ADULTO)' else x)\n",
        "\n",
        "    # Ensure 'CP' exists before processing\n",
        "    if 'CP' in df.columns:\n",
        "        df['CP'] = pd.to_numeric(df['CP'], errors='coerce').astype('Int64')\n",
        "\n",
        "    # Ensure 'YEAR NACIMIENTO' exists before processing\n",
        "    if 'FECHA NACIMIENTO' in df.columns:\n",
        "        df['YEAR NACIMIENTO'] = df['FECHA NACIMIENTO'].dt.year\n",
        "\n",
        "    # Check for 'LOCALIDAD' before attempting to drop or filter\n",
        "    if 'LOCALIDAD' in df.columns:\n",
        "        if 'CP' in df.columns:\n",
        "            df = df.dropna(subset=['CP', 'LOCALIDAD'], how='all')\n",
        "            CP_loc_filter = df['CP'].isnull() & df['LOCALIDAD'].isnull()\n",
        "            df = df[~CP_loc_filter]\n",
        "        else:\n",
        "            df = df.dropna(subset=['LOCALIDAD'], how='all')\n",
        "            loc_filter = df['LOCALIDAD'].isnull()\n",
        "            df = df[~loc_filter]\n",
        "\n",
        "    return df\n",
        "\n",
        "# DEJAR NO Y SI SOLO\n",
        "def norm_disc(val):\n",
        "    if pd.isna(val):\n",
        "        return val  # Retorna NaN tal como está\n",
        "    if isinstance(val, str):  # Verificar si el valor es una cadena\n",
        "        if val.startswith('SI'):\n",
        "            if '%' in val:\n",
        "                return int(val.split('(')[1].split('%')[0])  # Extraer el número del porcentaje\n",
        "            return 'SI'  # Convertir \"SI\" y variantes sin número a \"SI\"\n",
        "        if val.startswith('NO'):\n",
        "            return 'NO'  # Convertir \"NO TIENE\" y \"NO\" a \"NO\"\n",
        "        if 'PERM' in val:\n",
        "            return int(val.split()[0])  # Extraer el número antes de \"PERMANENTE\"\n",
        "        return val  # Retorna el valor tal cual si no se aplica ninguna condición anterior\n",
        "    return val  # Maneja números directamente si no son cadenas\n",
        "\n",
        "# DEJAR NO Y SI SOLO\n",
        "def norm_dep(value):\n",
        "    if pd.isna(value):\n",
        "        return value  # Retorna NaN tal como está\n",
        "    if isinstance(value, str):\n",
        "        value = value.strip()  # Limpiar espacios en blanco\n",
        "        if value.upper() == 'NO' or value.upper() == 'NO TIENE' or value == 'N0':\n",
        "            return 'NO'\n",
        "        elif value.upper() == 'SI':\n",
        "            return 'SI'\n",
        "        elif 'SI TIPO' in value:\n",
        "            return value.split('SI ')[1]  # Extraer solo la parte después de \"SI \"\n",
        "        else:\n",
        "            return value\n",
        "    elif isinstance(value, int):\n",
        "        if value in [1, 2, 3]:\n",
        "            return f'TIPO {value}'\n",
        "    return value\n",
        "\n",
        "def other_columns(df):\n",
        "  if 'PROTESIS' in df.columns:\n",
        "    df['PROTESIS'] = df['PROTESIS'].fillna('NO TIENE')\n",
        "    df['AUD O PROTESIS'] = np.where(df['PROTESIS'] == 'NO TIENE', 'NO', 'SI')\n",
        "    df.drop('PROTESIS', axis=1, inplace=True)\n",
        "\n",
        "  if 'TIPO SORDERA' in df.columns:\n",
        "    mapeo_sordera = {\n",
        "    'NEUROSENSORIAL': 'NEUROSENSORIAL',\n",
        "    'NEUSENSORIAL': 'NEUROSENSORIAL',\n",
        "    'HIPOACUSIA NEUROSENSORIAL': 'NEUROSENSORIAL',\n",
        "    'HIPOACUSIA NEUROSENSORIAL PROFUNDA': 'NEUROSENSORIAL',\n",
        "    'NEURO': 'NEUROSENSORIAL',\n",
        "    'NEURO ??': 'NEUROSENSORIAL',\n",
        "    'HIPOACUSIA MIXTA Y NEUROSENSORIAL': 'NEUROSENSORIAL',\n",
        "    'HIPOACUSIA ENDOCOCLEAR': 'NEUROSENSORIAL',\n",
        "\n",
        "    'CONDUCTIVA': 'CONDUCTIVA',\n",
        "    'HIPOACUSIA DE CONDUCCION': 'CONDUCTIVA',\n",
        "    'HIPOACUSIA CONDUCTIVA UNILATERAL': 'CONDUCTIVA',\n",
        "    'CONDUC': 'CONDUCTIVA',\n",
        "\n",
        "    'MIXTA': 'MIXTA',\n",
        "    'HIPOACUSIA MIXTA': 'MIXTA',\n",
        "\n",
        "    'HIPOACUSIA': 'HIPOACUSIA',\n",
        "\n",
        "    'DIFICULTADES DEL LENGUAJE': 'DIFICULTADES DEL LENGUAJE',\n",
        "    'RETRASO DEL LENGUAJE': 'RETRASO DEL LENGUAJE',\n",
        "\n",
        "    '-': 'INDEFINIDO',\n",
        "    'nan': 'INDEFINIDO'\n",
        "    }\n",
        "    df['TIPO SORDERA'] = df['TIPO SORDERA'].map(mapeo_sordera).fillna('INDEFINIDO')\n",
        "  if 'GRADO DISCAPACIDAD' in df.columns:\n",
        "    df['GRADO DISCAPACIDAD'] = df['GRADO DISCAPACIDAD'].apply(norm_disc)\n",
        "  if 'LOCALIDAD' in df.columns:\n",
        "    # Limpiar los datos eliminando contenido entre paréntesis\n",
        "    df['LOCALIDAD'] = df['LOCALIDAD'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
        "  if 'MOMENTO APARICION' in df.columns:\n",
        "    mapeo_apar = {\n",
        "        'PRE': 'PRELOCUTIVA',\n",
        "        'POST': 'POSTLOCUTIVA',\n",
        "        'PERI': 'PERILOCUTIVA',\n",
        "        '-': 'INDEFINIDO',\n",
        "        'nan': 'INDEFINIDO'\n",
        "    }\n",
        "    df['MOMENTO APARICION'] = df['MOMENTO APARICION'].map(mapeo_apar).fillna('INDEFINIDO')\n",
        "  if 'GRADO DEPENDENCIA' in df.columns:\n",
        "    df['GRADO DEPENDENCIA'] = df['GRADO DEPENDENCIA'].apply(norm_dep)\n",
        "  return df\n",
        "\n",
        "def year_conversion(df):\n",
        "  df['presente'] = 1\n",
        "\n",
        "  # Crear pivot table\n",
        "  pivot_df = df.pivot_table(index='FECHA NACIMIENTO', columns='ANO ATENCION', values='presente', fill_value=0, aggfunc='max')\n",
        "\n",
        "  # Restablecer el índice para hacer que id_persona sea una columna otra vez\n",
        "  pivot_df.reset_index(inplace=True)\n",
        "\n",
        "  df_original_clean = df.drop_duplicates(subset=['FECHA NACIMIENTO'])\n",
        "  df_final = pd.merge(df_original_clean, pivot_df, on=['FECHA NACIMIENTO'], how='left')\n",
        "\n",
        "  return df_final\n",
        "\n",
        "def convert_year_columns_to_int(df):\n",
        "    # Crear un nuevo diccionario para mapear nombres antiguos a nuevos\n",
        "    new_columns = {}\n",
        "    for col in df.columns:\n",
        "        # Intentar convertir la columna a entero si es posible\n",
        "        try:\n",
        "            # Convertir a entero si el nombre de la columna es numérico\n",
        "            new_columns[col] = int(col)\n",
        "        except ValueError:\n",
        "            # Mantener el nombre original si no es un año\n",
        "            new_columns[col] = col\n",
        "\n",
        "    # Renombrar las columnas usando el diccionario mapeado\n",
        "    df.rename(columns=new_columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "def CU_year_conversion(df):\n",
        "  if 'ANO ATENCION' in df.columns:\n",
        "      # Asegurar que todos los valores en 'ANO ATENCION' sean strings\n",
        "      df['ANO ATENCION'] = df['ANO ATENCION'].astype(str)\n",
        "\n",
        "      # Normalizar los datos: eliminar espacios y caracteres extraños\n",
        "      df['ANO ATENCION'] = df['ANO ATENCION'].apply(lambda x: x.replace(' ', '').replace('_', '').split('-'))\n",
        "\n",
        "      # Explode de los años de atención para preparar para get_dummies\n",
        "      df_exploded = df.explode('ANO ATENCION')\n",
        "\n",
        "      # Aplicar get_dummies para convertir años en columnas binarias\n",
        "      df_one_hot = pd.get_dummies(df_exploded['ANO ATENCION'].astype(str))\n",
        "\n",
        "      # Limpiar los nombres de columnas eliminando espacios y caracteres no deseados\n",
        "      df_one_hot.columns = df_one_hot.columns.str.replace('[^0-9]', '', regex=True).str.strip()\n",
        "\n",
        "      # Consolidar columnas duplicadas resultantes de la limpieza (suma)\n",
        "      df_one_hot = df_one_hot.groupby(df_one_hot.columns, axis=1).sum()\n",
        "\n",
        "      # Agregar las otras columnas para hacer un merge posteriormente\n",
        "      df_one_hot = pd.concat([df_exploded[['FECHA NACIMIENTO']], df_one_hot], axis=1)\n",
        "\n",
        "      # Agrupar por FECHA NACIMIENTO y sumar (esto asegura que solo habrá 0s y 1s)\n",
        "      df_final = df_one_hot.groupby(['FECHA NACIMIENTO']).sum().reset_index()\n",
        "\n",
        "      # Merge el DataFrame original con el DataFrame final transformado\n",
        "      df_merged = pd.merge(df, df_final, on=['FECHA NACIMIENTO'], how='left')\n",
        "      df_merged = convert_year_columns_to_int(df_merged)\n",
        "  else:\n",
        "      df_merged = df\n",
        "\n",
        "  return df_merged\n",
        "\n",
        "def keep_columns(df):\n",
        "    desired_columns = ['FECHA NACIMIENTO', 'GENERO', 'LOCALIDAD', 'CP', 'PROVINCIA','MOMENTO APARICION','TIPO SORDERA', 'GRADO PERDIDA', 'AUD O PROTESIS', 'GRADO DEPENDENCIA', 'GRADO DISCAPACIDAD', 'SISTEMA DE COMUNICACION',\n",
        "                       'YEAR NACIMIENTO', 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
        "    # Filtrar el DataFrame para incluir solo las columnas que existen en el DataFrame\n",
        "    filtered_df = df[df.columns.intersection(desired_columns)]\n",
        "    return filtered_df"
      ],
      "metadata": {
        "id": "Ic0XNdCMdRsY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que 'dict_of_dfs' es tu diccionario de DataFrames\n",
        "for df_key, df in dic_dataframes.items():\n",
        "    dic_dataframes[df_key] = transform_df(df)\n",
        "    dic_dataframes[df_key] = mapping_names(df)\n",
        "    dic_dataframes[df_key] = CP_year(df)\n",
        "    dic_dataframes[df_key] = other_columns(df)\n",
        "    dic_dataframes[df_key] = CU_year_conversion(df)\n",
        "    dic_dataframes[df_key] = keep_columns(df)"
      ],
      "metadata": {
        "id": "NA5jA9TkdgdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CU_LOG = dic_dataframes.get(\"CU_LOG.xlsx\")\n",
        "TO_LOG = dic_dataframes.get(\"TO_LOG.xlsx\")\n",
        "AB_LOG = dic_dataframes.get(\"AB_LOG.xlsx\")\n",
        "GU_LOG = dic_dataframes.get(\"GU_LOG.xlsx\")\n",
        "CR_LOG = dic_dataframes.get(\"CR_LOG.xlsx\")"
      ],
      "metadata": {
        "id": "krrRWEQZgBBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OJO CR NO TIENE CP NI LOCALIDAD, NO PUEDO CRUZAR LOS DATOS CON NADA**"
      ],
      "metadata": {
        "id": "AZ2lBiqOhiOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # CÓDIGO DE USO PARA EL CURRO\n",
        "CU_LOG = transform_df(CU_LOG)\n",
        "CU_LOG = mapping_names(CU_LOG)\n",
        "CU_LOG = CP_year(CU_LOG)\n",
        "CU_LOG = other_columns(CU_LOG)\n",
        "CU_LOG = CU_year_conversion(CU_LOG)\n",
        "CU_LOG = keep_columns(CU_LOG)\n",
        "\n",
        "# GU_LOG = transform_df(GU_LOG)\n",
        "# GU_LOG = mapping_names(GU_LOG)\n",
        "# GU_LOG = CP_year(GU_LOG)\n",
        "# GU_LOG = other_columns(GU_LOG)\n",
        "# GU_LOG = year_conversion(GU_LOG)\n",
        "# GU_LOG = keep_columns(GU_LOG)\n",
        "\n",
        "# TO_LOG = transform_df(TO_LOG)\n",
        "# TO_LOG = mapping_names(TO_LOG)\n",
        "# TO_LOG = CP_year(TO_LOG)\n",
        "# TO_LOG = other_columns(TO_LOG)\n",
        "# TO_LOG = year_conversion(TO_LOG)\n",
        "# TO_LOG = keep_columns(TO_LOG)\n",
        "\n",
        "# CR_LOG = transform_df(CR_LOG)\n",
        "# CR_LOG = mapping_names(CR_LOG)\n",
        "# # CR_LOG = CP_year(CR_LOG)\n",
        "# CR_LOG = other_columns(CR_LOG)\n",
        "# CR_LOG = year_conversion(CR_LOG)\n",
        "# CR_LOG = keep_columns(CR_LOG)"
      ],
      "metadata": {
        "id": "_0YtPJ-rdlgX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR QUÉ APARECEN AÑOS X E Y???**"
      ],
      "metadata": {
        "id": "-Rk3D0T3rHRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLM_LOG = pd.concat([CU_LOG, CR_LOG, GU_LOG, TO_LOG])"
      ],
      "metadata": {
        "id": "sSA3mQWcQM7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of DataFrames\n",
        "dfs = [CR_LOG, CU_LOG, GU_LOG, TO_LOG, CLM_LOG]\n",
        "\n",
        "# Corresponding folder paths on Google Drive\n",
        "folder_paths = [\n",
        "    '/content/drive/My Drive/PFG_FASPAS/CR',\n",
        "    '/content/drive/My Drive/PFG_FASPAS/CU',\n",
        "    '/content/drive/My Drive/PFG_FASPAS/GU',\n",
        "    '/content/drive/My Drive/PFG_FASPAS/TO',\n",
        "    '/content/drive/My Drive/PFG_FASPAS/CLM'\n",
        "]\n",
        "\n",
        "# Corresponding file names\n",
        "file_names = ['CR_LOG_limpio.xlsx', 'CU_LOG_limpio.xlsx', 'GU_LOG_limpio.xlsx', 'TO_LOG_limpio.xlsx', 'CLM_LOG_limpio.xlsx']"
      ],
      "metadata": {
        "id": "2MbuE0k7gS2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the DataFrames, folder paths, and file names\n",
        "for df, folder_path, file_name in zip(dfs, folder_paths, file_names):\n",
        "    # Define the complete file path\n",
        "    file_path = f\"{folder_path}/{file_name}\"\n",
        "\n",
        "    # Save the DataFrame as an Excel file\n",
        "    df.to_excel(file_path, index=False)\n",
        "\n",
        "print(\"All DataFrames have been exported successfully.\")"
      ],
      "metadata": {
        "id": "UiZ1aN4ngvP0",
        "outputId": "46482fd0-37fd-4203-8703-b885c3ab3057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All DataFrames have been exported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargarlo en el curro\n",
        "CU_LOG.to_excel('CU_LOG_limpio.xlsx', index=False, engine='openpyxl')"
      ],
      "metadata": {
        "id": "yrW-p5BqVfS4"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}