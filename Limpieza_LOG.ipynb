{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Limpieza servicio logopedia\n",
        "#### **NO hay datos de ALBACETE**\n"
      ],
      "metadata": {
        "id": "cnOLRgPUbszf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RpuopuAEb6ta"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLTdD1uocEfl",
        "outputId": "16e34ea5-fb25-4394-9501-d08e57a8b12e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unidecode import unidecode\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rQ0dgAzxb9Dg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "# folder_path = '/content/drive/My Drive/PFG_FASPAS/SAAF'\n",
        "# docs_xlsx = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
        "# print(docs_xlsx)\n",
        "\n",
        "# # Asegúrarse de cambiar la ruta por el nombre real de la carpeta en Google Drive\n",
        "# folder_path_csv = '/content/drive/My Drive/PFG_FASPAS/Zonas_Prioritarias'\n",
        "# docs_csv = [f for f in os.listdir(folder_path_csv) if f.endswith('.csv')]\n",
        "# print(docs_csv)"
      ],
      "metadata": {
        "id": "Blt7RvhgcG7d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dic_dataframes = {}\n",
        "# dic_zonas = {}\n",
        "\n",
        "# for doc in docs_xlsx:\n",
        "#     entire_path = os.path.join(folder_path, doc)\n",
        "#     df = pd.read_excel(entire_path)\n",
        "#     # Uso el nombre del archivo como clave\n",
        "#     dic_dataframes[doc] = df\n",
        "\n",
        "# for doc in docs_csv:\n",
        "#   entire_path = os.path.join(folder_path_csv, doc)\n",
        "#   df = pd.read_csv(entire_path)\n",
        "#   # Uso el nombre del archivo como clave\n",
        "#   dic_zonas[doc] = df"
      ],
      "metadata": {
        "id": "ssYdy0LvcIQc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Imprimir todas las claves\n",
        "# for clave in dic_dataframes.keys():\n",
        "#     print(clave)"
      ],
      "metadata": {
        "id": "c_PeZbZecKca"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CU_SAAF = dic_dataframes.get(\"CU_SAAF.xlsx\")\n",
        "# CU_AT = dic_dataframes.get(\"CU_AT.xlsx\")\n",
        "# CU_EMPLEO = dic_dataframes.get(\"CU_EMPLEO.xlsx\")\n",
        "# CU_LOG = dic_dataframes.get(\"CU_LOG.xlsx\")\n",
        "# # CU_SOCIOS = dic_dataframes.get(\"CU_SOCIOS.xlsx\")"
      ],
      "metadata": {
        "id": "Kfti6G8gcLua"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÓDIGO SOLO PARA EL CURRO (PQ NO PUEDO ACCEDER AL DRIVE)\n",
        "CU_LOG = pd.read_excel('/content/CU_LOG.xlsx')\n",
        "TO_LOG = pd.read_excel('/content/TO_LOG.xlsx')\n",
        "# AB_LOG = pd.read_excel('/content/AB_LOG.xlsx')\n",
        "GU_LOG = pd.read_excel('/content/GU_LOG.xlsx')\n",
        "CR_LOG = pd.read_excel('/content/CR_LOG.xlsx')"
      ],
      "metadata": {
        "id": "iCWw6s7zcPdR"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función que realiza las transformaciones\n",
        "def transform_df(df):\n",
        "    col_excluded = ['FECHA NACIMIENTO', 'Fecha nacimiento', 'Inicio Tratamiento', 'FECHA INICIO/FECHA REVISIÓN']\n",
        "\n",
        "    # Convertir todas las columnas de tipo object a mayúsculas, excepto las especificadas\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'object' and column not in col_excluded:\n",
        "            df[column] = df[column].apply(lambda x: unidecode(x.upper()) if isinstance(x, str) else x)\n",
        "\n",
        "    # Ahora, aplicamos unidecode a los nombres de las columnas\n",
        "    df.columns = [unidecode(col.upper()) for col in df.columns]\n",
        "\n",
        "    return df\n",
        "\n",
        "def mapping_names(df):\n",
        "    # Diccionario con los mapeos deseados\n",
        "    columns_map = {\n",
        "        'C. POSTAL': 'CP',\n",
        "        'MOMENTO DE APARICION DE LA SORDERA': 'MOMENTO APARICION',\n",
        "        'SIST. COMUNICACION': 'SISTEMA COMUNICACION',\n",
        "        'TIPO': 'TIPO SORDERA',\n",
        "        'TIPO HIPOACUSIA': 'TIPO SORDERA',\n",
        "        'GRADO': 'GRADO SORDERA',\n",
        "        'TIPO PROTESIS': 'PROTESIS',\n",
        "        'AUDIF/I.C': 'PROTESIS',\n",
        "        'FECHA INICIO/FECHA REVISION': 'INICIO TRATAMIENTO'\n",
        "    }\n",
        "\n",
        "    # Crear un nuevo diccionario para los nombres de columnas\n",
        "    rename_columns = {}\n",
        "\n",
        "    # Iterar sobre las columnas y aplicar el mapeo\n",
        "    for col in df.columns:\n",
        "        norm_col = columns_map.get(col, col)\n",
        "        rename_columns[col] = norm_col\n",
        "\n",
        "    # Renombrar las columnas del DataFrame\n",
        "    df.rename(columns=rename_columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "def CP_year(df):\n",
        "  if not pd.api.types.is_datetime64_any_dtype(df['FECHA NACIMIENTO']):\n",
        "    df['FECHA NACIMIENTO'] = pd.to_datetime(df['FECHA NACIMIENTO'], errors='coerce')\n",
        "\n",
        "  df['FECHA NACIMIENTO'] = df['FECHA NACIMIENTO'].apply(lambda x: np.nan if str(x).isdigit() or x == '(ADULTO)' else x)\n",
        "  # df['FECHA NACIMIENTO'] = pd.to_datetime(df['FECHA NACIMIENTO'], errors='coerce')\n",
        "  if 'CP' in df.columns:\n",
        "    df['CP'] =df['CP'].astype('Int64')\n",
        "\n",
        "    df['YEAR NACIMIENTO'] = df['FECHA NACIMIENTO'].dt.year\n",
        "\n",
        "    df = df.dropna(subset=['CP', 'LOCALIDAD'], how='all')\n",
        "\n",
        "    CP_loc_filter = df['CP'].isnull() & df['LOCALIDAD'].isnull()\n",
        "\n",
        "    df = df[~CP_loc_filter]\n",
        "  else:\n",
        "    df['YEAR NACIMIENTO'] = df['FECHA NACIMIENTO'].dt.year\n",
        "\n",
        "    df = df.dropna(subset=['LOCALIDAD'], how='all')\n",
        "\n",
        "    loc_filter = df['LOCALIDAD'].isnull()\n",
        "\n",
        "    df = df[~loc_filter]\n",
        "  return df\n",
        "\n",
        "def other_columns(df):\n",
        "  if 'PROTESIS' in df.columns:\n",
        "    df['PROTESIS'] = df['PROTESIS'].fillna('NO TIENE')\n",
        "    df['AUD O PROTESIS'] = np.where(df['PROTESIS'] == 'NO TIENE', 'NO', 'SI')\n",
        "    df.drop('PROTESIS', axis=1, inplace=True)\n",
        "  return df\n",
        "\n",
        "def year_conversion(df):\n",
        "  df['presente'] = 1\n",
        "\n",
        "  # Crear pivot table\n",
        "  pivot_df = df.pivot_table(index='FECHA NACIMIENTO', columns='ANO ATENCION', values='presente', fill_value=0, aggfunc='max')\n",
        "\n",
        "  # Restablecer el índice para hacer que id_persona sea una columna otra vez\n",
        "  pivot_df.reset_index(inplace=True)\n",
        "\n",
        "  df_original_clean = GU_LOG.drop_duplicates(subset=['FECHA NACIMIENTO'])\n",
        "  df_final = pd.merge(df_original_clean, pivot_df, on=['FECHA NACIMIENTO'], how='left')\n",
        "  # df_final.drop('presente', axis=1, inplace=True)\n",
        "\n",
        "  return df_final\n",
        "\n",
        "def CU_year_conversion(df):\n",
        "  # Separar los años de atención y expandir en una lista\n",
        "  df['ANO ATENCION'] = df['ANO ATENCION'].str.split(' - ')\n",
        "  # Explode de los años de atención para preparar para get_dummies\n",
        "  df_exploded = df.explode('ANO ATENCION')\n",
        "\n",
        "  # Aplicar get_dummies para convertir años en columnas binarias\n",
        "  df_one_hot = pd.get_dummies(df_exploded['ANO ATENCION'].astype(str), prefix='')\n",
        "\n",
        "  # Agregar las otras columnas para hacer un merge posteriormente\n",
        "  df_one_hot = pd.concat([df_exploded[['FECHA NACIMIENTO']], df_one_hot], axis=1)\n",
        "\n",
        "  # Agrupar por id_persona y fecha_nacimiento y sumar (esto asegura que solo habrá 0s y 1s)\n",
        "  df_final = df_one_hot.groupby(['FECHA NACIMIENTO']).sum().reset_index()\n",
        "\n",
        "  # Asegurar que el df_final ya está definido como mostré en la última parte de la transformación\n",
        "  # Merge el DataFrame original con el DataFrame final transformado\n",
        "  df_merged = pd.merge(df, df_final, on=['FECHA NACIMIENTO'], how='left')\n",
        "\n",
        "  return df_merged"
      ],
      "metadata": {
        "id": "Ic0XNdCMdRsY"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Suponiendo que 'dict_of_dfs' es tu diccionario de DataFrames\n",
        "# for df_key, df in dic_dataframes.items():\n",
        "#     dic_dataframes[df_key] = transform_df(df)\n",
        "#     dic_dataframes[df_key] = mapping_names(df)\n",
        "#     dic_dataframes[df_key] = kept_columns(df)\n",
        "#     dic_dataframes[df_key] = CP_year(df)\n",
        "#     dic_dataframes[df_key] = other_columns(df)\n",
        "#     dic_dataframes[df_key] = delete_not_important_columns(df)\n",
        "#     dic_dataframes[df_key] = left_columns_norm(df)"
      ],
      "metadata": {
        "id": "NA5jA9TkdgdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OJO CR NO TIENE CP NI LOCALIDAD, NO PUEDO CRUZAR LOS DATOS CON NADA**"
      ],
      "metadata": {
        "id": "AZ2lBiqOhiOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CÓDIGO DE USO PARA EL CURRO\n",
        "CU_LOG = transform_df(CU_LOG)\n",
        "CU_LOG = mapping_names(CU_LOG)\n",
        "CU_LOG = CP_year(CU_LOG)\n",
        "CU_LOG = other_columns(CU_LOG)\n",
        "CU_LOG = CU_year_conversion(CU_LOG)\n",
        "\n",
        "GU_LOG = transform_df(GU_LOG)\n",
        "GU_LOG = mapping_names(GU_LOG)\n",
        "GU_LOG = CP_year(GU_LOG)\n",
        "GU_LOG = other_columns(GU_LOG)\n",
        "GU_LOG = year_conversion(GU_LOG)\n",
        "\n",
        "TO_LOG = transform_df(TO_LOG)\n",
        "TO_LOG = mapping_names(TO_LOG)\n",
        "TO_LOG = CP_year(TO_LOG)\n",
        "TO_LOG = other_columns(TO_LOG)\n",
        "TO_LOG = year_conversion(TO_LOG)\n",
        "\n",
        "CR_LOG = transform_df(CR_LOG)\n",
        "CR_LOG = mapping_names(CR_LOG)\n",
        "# CR_LOG = CP_year(CR_LOG)\n",
        "CR_LOG = other_columns(CR_LOG)\n",
        "CR_LOG = year_conversion(CR_LOG)"
      ],
      "metadata": {
        "id": "_0YtPJ-rdlgX"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POR QUÉ APARECEN AÑOS X E Y???**"
      ],
      "metadata": {
        "id": "-Rk3D0T3rHRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CÓDIGO DE USO PARA EL CURRO\n",
        "GU_LOG.info()\n",
        "column_names = GU_LOG.columns.tolist()\n",
        "\n",
        "for column in column_names:\n",
        "  unique_val = GU_LOG[column].unique()\n",
        "  print(unique_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gztTahRrgCmX",
        "outputId": "619b1215-6ee0-4c18-d0bd-40fb4fdcc794"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24 entries, 0 to 23\n",
            "Data columns (total 19 columns):\n",
            " #   Column                Non-Null Count  Dtype         \n",
            "---  ------                --------------  -----         \n",
            " 0   GENERO                24 non-null     object        \n",
            " 1   FECHA NACIMIENTO      23 non-null     datetime64[ns]\n",
            " 2   MOMENTO APARICION     24 non-null     object        \n",
            " 3   GRADO SORDERA         24 non-null     object        \n",
            " 4   TIPO SORDERA          24 non-null     object        \n",
            " 5   SISTEMA COMUNICACION  24 non-null     object        \n",
            " 6   CAUSA                 24 non-null     object        \n",
            " 7   LOCALIDAD             24 non-null     object        \n",
            " 8   CP                    24 non-null     Int64         \n",
            " 9   GRADO DISCAPACIDAD    24 non-null     object        \n",
            " 10  GRADO DEPENDENCIA     24 non-null     object        \n",
            " 11  ANO ATENCION          24 non-null     int64         \n",
            " 12  YEAR NACIMIENTO       23 non-null     float64       \n",
            " 13  AUD O PROTESIS        24 non-null     object        \n",
            " 14  presente              24 non-null     int64         \n",
            " 15  2020                  23 non-null     float64       \n",
            " 16  2021                  23 non-null     float64       \n",
            " 17  2022                  23 non-null     float64       \n",
            " 18  2023                  23 non-null     float64       \n",
            "dtypes: Int64(1), datetime64[ns](1), float64(5), int64(2), object(10)\n",
            "memory usage: 3.7+ KB\n",
            "['HOMBRE' 'MUJER']\n",
            "<DatetimeArray>\n",
            "['2010-12-17 00:00:00', '2007-12-10 00:00:00', '2007-09-19 00:00:00',\n",
            " '2017-06-19 00:00:00', '2013-04-05 00:00:00', '2015-04-28 00:00:00',\n",
            " '2011-12-13 00:00:00', '1954-11-02 00:00:00', '2008-10-31 00:00:00',\n",
            " '2010-04-15 00:00:00', '2018-05-06 00:00:00', '2016-04-14 00:00:00',\n",
            " '2013-09-17 00:00:00', '1964-01-11 00:00:00', '2020-03-21 00:00:00',\n",
            " '2015-09-12 00:00:00', '2014-07-22 00:00:00', '2015-12-27 00:00:00',\n",
            " '2012-10-29 00:00:00', '2015-07-03 00:00:00', '2015-10-04 00:00:00',\n",
            "                 'NaT', '1968-10-18 00:00:00', '1999-06-28 00:00:00']\n",
            "Length: 24, dtype: datetime64[ns]\n",
            "['PRE' 'PERI' 'POST' '-']\n",
            "['PROFUNDA' 'SEVERA' 'MODERADA' 'LEVE ??' '-' 'LEVE']\n",
            "['NEURO' 'NEURO ??' 'MIXTA' '-' 'CONDUC']\n",
            "['ORAL' 'ORAL- BIMODAL' 'SIGNOS' 'ORAL/SIGNOS']\n",
            "['DESCONOCIDA' 'MENINGITIS' 'POSIBLE CONGENITA' 'POSIBLE MOCOS'\n",
            " 'ANTECEDENTES FAMILIARES' 'MEDICAMENTO OTOTOXICO' 'POSIBLES ANTECEDENTES'\n",
            " 'ENFERMEDAD MENIERE' 'POSIBLE MUTACION' '-' 'SINDROME DE X FRAGIL'\n",
            " 'ENFERMEDAD DEGENERATIVA']\n",
            "['MARCHAMALO' 'FONTANAR' 'QUER' 'YUNQUERA' 'GUADALAJARA' 'CHILOECHES'\n",
            " 'AZUQUECA DE HENARES' 'CABANILLAS DEL CAMPO' 'ALOVERA']\n",
            "<IntegerArray>\n",
            "[19180, 19290, 19209, 19210, 19005, 19160, 19004, 19003, 19002, 19200, 19171,\n",
            " 19208]\n",
            "Length: 12, dtype: Int64\n",
            "[56 51 55 54 39 36 '37 (PERMANTE)' '33 (PERMANENTE)' 66 46 'NO TIENE' 21\n",
            " 40 67 41 42 33]\n",
            "[2 1 'NO TIENE' 3]\n",
            "[2023 2022 2021 2020]\n",
            "[2010. 2007. 2017. 2013. 2015. 2011. 1954. 2008. 2018. 2016. 1964. 2020.\n",
            " 2014. 2012.   nan 1968. 1999.]\n",
            "['SI']\n",
            "[1]\n",
            "[ 1.  0. nan]\n",
            "[ 1.  0. nan]\n",
            "[ 1.  0. nan]\n",
            "[ 1.  0. nan]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}